{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a54de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import JQL, FIELDS, BASE_URL, JIRA_DOMAIN,EMAIL, MAX_RESULTS, MODULE_DEVS, VALID_STATUSES, MAIL_MAP, DAILY_HOURS,MIN_PROJECT_RATIO,PROJECT_MAP, DEFAULT_END_DATE, DEFAULT_END_DATE_with_timezone, DEFAULT_START_DATE,DEFAULT_START_DATE_with_timezone\n",
    "from token_hidden import API_TOKEN\n",
    "SPRINT_A_CALCULAR = \"S20251027\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a13c83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Actualizado ./bt_it_sprint_planning.json para sprint S20251027\n",
      "Semana 1: {'tarjetas_planificadas': 14, 'horas_planificadas': 101.0}\n",
      "Semana 2: {'tarjetas_planificadas': 20, 'horas_planificadas': 174.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_14852\\1907969500.py:111: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Recolector de planificación (IT/BT) desde Google Sheets.\n",
    "\n",
    "- Lee la hoja (ID + GID).\n",
    "- Detecta el sprint actual (col 'Sprint' o escaneando SYYYYMMDD).\n",
    "- Por desarrollador:\n",
    "    * Inicio real de sprint: min('Fecha inicio sprint') o fallback por código de sprint.\n",
    "    * Fin real de sprint: max('Fecha fin sprint') o fallback = inicio + 11 días (23:59:59).\n",
    "    * Filtra tarjetas con 'Fecha Estimada' (columna AQ o header) ∈ [inicio, fin].\n",
    "    * Separa en week1 (lun–vie primera) y week2 (lun segunda → fin real).\n",
    "    * Suma tarjetas y horas planificadas y guarda detalle por issue.\n",
    "\n",
    "Salida: bt_it_sprint_planning.json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =======================\n",
    "# ===== CONFIG INICIO ===\n",
    "# =======================\n",
    "\n",
    "SHEET_ID = \"1NW5uT16Req72uMfYZ6-eITvwk9uKvXncEYsqWou9-D0\"\n",
    "WORKSHEET_GID = 985465980\n",
    "CREDS_PATH = \"./service_account.json\"\n",
    "OUTPUT_JSON = \"./bt_it_sprint_planning.json\"\n",
    "\n",
    "# Fecha de entrega:\n",
    "DELIVERY_DATE_HEADER = \"Fecha Estimada\"\n",
    "FORCE_COLUMN_LETTER = \"AQ\"   # forzar AQ como “Fecha Estimada”\n",
    "\n",
    "# Columnas para horas planificadas (primera que exista)\n",
    "ESTIMATE_CANDIDATES = [\n",
    "    \"Estimacion\", \"Estimación\", \"Estimación Personal\", \"Story point estimate\"\n",
    "]\n",
    "\n",
    "SPRINT_COL = \"Sprint\"\n",
    "DEV_COL_CANDIDATES = [\"Desarrollador\", \"Persona asignada\"]\n",
    "SPRINT_START_HEADER = \"Fecha inicio sprint\"\n",
    "CARD_START_HEADER   = \"Fecha inicio tarjeta\"\n",
    "SPRINT_END_HEADER   = \"Fecha fin sprint\"\n",
    "KEY_COL_CANDIDATES  = [\"Clave\", \"ID\"]  # “Clave” en tu Sheet\n",
    "TYPE_COL            = \"Tipo\"\n",
    "\n",
    "# =======================\n",
    "# ===== CONFIG FIN ======\n",
    "# =======================\n",
    "\n",
    "\n",
    "# -------- Google Sheets helpers --------\n",
    "def fetch_sheet_with_service_account_by_gid(\n",
    "    sheet_id: str,\n",
    "    worksheet_gid: int,\n",
    "    creds_path: str,\n",
    "    header_row: int = 3,      # header en fila 3\n",
    "    first_data_row: int = 4,  # datos a partir de fila 4 (puede haber huecos)\n",
    ") -> pd.DataFrame:\n",
    "    import gspread\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "    scope = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "        \"https://www.googleapis.com/auth/drive.readonly\",\n",
    "    ]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(creds_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    sh = client.open_by_key(sheet_id)\n",
    "    ws = sh.get_worksheet_by_id(worksheet_gid)\n",
    "    if ws is None:\n",
    "        raise RuntimeError(f\"No encontré worksheet con gid={worksheet_gid}\")\n",
    "\n",
    "    values = ws.get_all_values()\n",
    "    if not values or len(values) < header_row:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    raw_headers = values[header_row - 1]\n",
    "    rows = values[first_data_row - 1:]\n",
    "\n",
    "    def norm(h):\n",
    "        return (h or \"\").replace(\"\\u00a0\", \" \").strip()\n",
    "\n",
    "    headers, seen = [], {}\n",
    "    for idx, h in enumerate(raw_headers, start=1):\n",
    "        name = norm(h) or f\"__blank_{idx}\"\n",
    "        base = name\n",
    "        n = seen.get(base, 0)\n",
    "        if n:\n",
    "            name = f\"{base}_{n+1}\"\n",
    "        seen[base] = n + 1\n",
    "        headers.append(name)\n",
    "\n",
    "    norm_rows = [\n",
    "        r + [\"\"] * (len(headers) - len(r)) if len(r) < len(headers) else r[:len(headers)]\n",
    "        for r in rows\n",
    "    ]\n",
    "    df = pd.DataFrame(norm_rows, columns=headers).dropna(axis=1, how=\"all\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def strip_sheet_gaps(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = df.replace(\"\\u00a0\", \" \", regex=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    empty_all = df.isna().all(axis=1) | df.apply(lambda r: all(str(x).strip() == \"\" for x in r), axis=1)\n",
    "    df = df.loc[~empty_all].copy()\n",
    "\n",
    "    if SPRINT_COL in df.columns:\n",
    "        s = df[SPRINT_COL].astype(str).str.strip()\n",
    "        if (s != \"\").any():\n",
    "            first_idx = s.ne(\"\").idxmax()\n",
    "            df = df.loc[first_idx:].copy()\n",
    "\n",
    "    empty_all = df.isna().all(axis=1) | df.apply(lambda r: all(str(x).strip() == \"\" for x in r), axis=1)\n",
    "    df = df.loc[~empty_all].copy()\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------- General helpers --------\n",
    "def column_letter_to_index(letter: str) -> int:\n",
    "    letter = letter.strip().upper()\n",
    "    result = 0\n",
    "    for ch in letter:\n",
    "        result = result * 26 + (ord(ch) - ord('A') + 1)\n",
    "    return result - 1\n",
    "\n",
    "def parse_date_series(series_like, dayfirst=True) -> pd.Series:\n",
    "    s = pd.Series(series_like)\n",
    "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst)\n",
    "\n",
    "def to_float_series(series_like) -> pd.Series:\n",
    "    s = pd.Series(series_like)\n",
    "    def _coerce(x):\n",
    "        if x is None:\n",
    "            return 0.0\n",
    "        sx = str(x).strip().replace(\",\", \".\")\n",
    "        if sx == \"\":\n",
    "            return 0.0\n",
    "        try:\n",
    "            return float(sx)\n",
    "        except:\n",
    "            return 0.0\n",
    "    return s.map(_coerce)\n",
    "\n",
    "def pick_dev_column(df: pd.DataFrame) -> str:\n",
    "    for c in DEV_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if \"asignad\" in c.lower() or \"desarrollador\" in c.lower():\n",
    "            return c\n",
    "    raise RuntimeError(\"No se encontró columna de desarrollador.\")\n",
    "\n",
    "def pick_key_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in KEY_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if c.strip().lower() in {\"clave\", \"key\", \"issue\", \"ticket\"}:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_current_sprint(df: pd.DataFrame, sprint_col: str, today: datetime) -> Tuple[str, datetime]:\n",
    "    def _pick_best(candidates: List[Tuple[str, datetime]]) -> Tuple[str, datetime]:\n",
    "        le_today = [c for c in candidates if c[1] <= today]\n",
    "        return max(le_today, key=lambda x: x[1]) if le_today else max(candidates, key=lambda x: x[1])\n",
    "\n",
    "    candidates: List[Tuple[str, datetime]] = []\n",
    "\n",
    "    if sprint_col in df.columns:\n",
    "        for val in df[sprint_col].dropna().astype(str).unique():\n",
    "            m = re.match(r\"^S(\\d{8})$\", val.strip())\n",
    "            if m:\n",
    "                d = datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "                candidates.append((val.strip(), d))\n",
    "        if candidates:\n",
    "            return _pick_best(candidates)\n",
    "\n",
    "    for col in df.columns:\n",
    "        for val in df[col].dropna().astype(str).unique():\n",
    "            s = val.strip()\n",
    "            m = re.match(r\"^S(\\d{8})$\", s)\n",
    "            if m:\n",
    "                d = datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "                candidates.append((s, d))\n",
    "    if candidates:\n",
    "        return _pick_best(candidates)\n",
    "\n",
    "    # Derivar desde fecha de inicio (global)\n",
    "    start_candidates = []\n",
    "    if SPRINT_START_HEADER in df.columns:\n",
    "        s = parse_date_series(df[SPRINT_START_HEADER]).dropna()\n",
    "        if not s.empty:\n",
    "            start_candidates.append(s.min())\n",
    "    if not start_candidates and CARD_START_HEADER in df.columns:\n",
    "        s = parse_date_series(df[CARD_START_HEADER]).dropna()\n",
    "        if not s.empty:\n",
    "            start_candidates.append(s.min())\n",
    "    base = start_candidates[0] if start_candidates else today\n",
    "    base = datetime(base.year, base.month, base.day)\n",
    "    sprint_id = f\"S{base.strftime('%Y%m%d')}\"\n",
    "    return sprint_id, base\n",
    "\n",
    "def compute_dev_sprint_start(dev_df: pd.DataFrame, default_start: datetime) -> datetime:\n",
    "    starts = []\n",
    "    if SPRINT_START_HEADER in dev_df.columns:\n",
    "        s = parse_date_series(dev_df[SPRINT_START_HEADER]).dropna()\n",
    "        if not s.empty: starts.append(s.min())\n",
    "    if not starts and CARD_START_HEADER in dev_df.columns:\n",
    "        s = parse_date_series(dev_df[CARD_START_HEADER]).dropna()\n",
    "        if not s.empty: starts.append(s.min())\n",
    "    if not starts:\n",
    "        return default_start\n",
    "    dt = starts[0]\n",
    "    return datetime(dt.year, dt.month, dt.day)\n",
    "\n",
    "def compute_dev_sprint_end(dev_df: pd.DataFrame, default_start: datetime) -> datetime:\n",
    "    if SPRINT_END_HEADER in dev_df.columns:\n",
    "        s = parse_date_series(dev_df[SPRINT_END_HEADER]).dropna()\n",
    "        if not s.empty:\n",
    "            d = s.max().to_pydatetime()\n",
    "            return datetime(d.year, d.month, d.day, 23, 59, 59)\n",
    "    return default_start + timedelta(days=11, hours=23, minutes=59, seconds=59)\n",
    "\n",
    "def choose_delivery_series(df: pd.DataFrame) -> pd.Series:\n",
    "    if DELIVERY_DATE_HEADER in df.columns:\n",
    "        return parse_date_series(df[DELIVERY_DATE_HEADER])\n",
    "    if FORCE_COLUMN_LETTER:\n",
    "        col_idx = column_letter_to_index(FORCE_COLUMN_LETTER)\n",
    "        if col_idx < len(df.columns):\n",
    "            s = df.iloc[:, col_idx]\n",
    "            return parse_date_series(s)\n",
    "        raise RuntimeError(f\"FORCE_COLUMN_LETTER={FORCE_COLUMN_LETTER} excede el ancho de la hoja.\")\n",
    "    raise RuntimeError(\"No encontré la columna de fecha de entrega.\")\n",
    "\n",
    "def choose_estimate_series(df: pd.DataFrame) -> pd.Series:\n",
    "    for c in ESTIMATE_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return to_float_series(df[c])\n",
    "    return pd.Series([0.0] * len(df), index=df.index)\n",
    "\n",
    "def within(ts: pd.Timestamp, start: datetime, end: datetime) -> bool:\n",
    "    if pd.isna(ts): return False\n",
    "    d = ts.to_pydatetime()\n",
    "    return start <= d <= end\n",
    "\n",
    "def build_week_windows(sprint_start: datetime, sprint_end: datetime) -> Dict[str, Tuple[datetime, datetime]]:\n",
    "    # Semana 1: lunes → viernes\n",
    "    w1_end = sprint_start + timedelta(days=4, hours=23, minutes=59, seconds=59)\n",
    "    # Semana 2: lunes siguiente → viernes siguiente\n",
    "    w2_start = sprint_start + timedelta(days=7)\n",
    "    w2_end = sprint_start + timedelta(days=11, hours=23, minutes=59, seconds=59)\n",
    "\n",
    "    # Evitar pasarse del sprint_end real (por si termina antes)\n",
    "    w1_end = min(w1_end, sprint_end)\n",
    "    w2_end = min(w2_end, sprint_end)\n",
    "\n",
    "    return {\n",
    "        \"all\": (sprint_start, sprint_end),\n",
    "        \"week1\": (w1_end - timedelta(days=4), w1_end),\n",
    "        \"week2\": (w2_start, w2_end)\n",
    "    }\n",
    "\n",
    "\n",
    "# -------- Core --------\n",
    "def compute_metrics_for_sprint(df: pd.DataFrame, today: datetime) -> Dict:\n",
    "    sprint_id, sprint_code_date = detect_current_sprint(df, SPRINT_COL, today)\n",
    "    current = df[df[SPRINT_COL].astype(str).str.strip() == sprint_id].copy()\n",
    "    if current.empty:\n",
    "        raise RuntimeError(f\"No hay filas para el sprint {sprint_id}\")\n",
    "\n",
    "    dev_col = pick_dev_column(current)\n",
    "    key_col = pick_key_column(current)\n",
    "    delivery_series = choose_delivery_series(current)\n",
    "    estimate_series = choose_estimate_series(current)\n",
    "    tipo_col = TYPE_COL if TYPE_COL in current.columns else None\n",
    "\n",
    "    out_by_dev: Dict[str, Dict] = {}\n",
    "    totals_w1_cards = totals_w1_hours = 0.0\n",
    "    totals_w2_cards = totals_w2_hours = 0.0\n",
    "    global_starts, global_ends = [], []\n",
    "\n",
    "    for dev, dev_df in current.groupby(dev_col):\n",
    "        dev_df = dev_df.copy()\n",
    "        dev_delivery = delivery_series.loc[dev_df.index]\n",
    "        dev_est = estimate_series.loc[dev_df.index]\n",
    "\n",
    "        sprint_start = compute_dev_sprint_start(dev_df, sprint_code_date)\n",
    "        sprint_end   = compute_dev_sprint_end(dev_df, sprint_start)\n",
    "        windows = build_week_windows(sprint_start, sprint_end)\n",
    "\n",
    "        global_starts.append(sprint_start)\n",
    "        global_ends.append(sprint_end)\n",
    "\n",
    "        w1_mask = dev_delivery.apply(lambda x: within(x, *windows[\"week1\"]))\n",
    "        w2_mask = dev_delivery.apply(lambda x: within(x, *windows[\"week2\"]))\n",
    "\n",
    "        w1_cards = int(w1_mask.sum())\n",
    "        w2_cards = int(w2_mask.sum())\n",
    "\n",
    "        w1_hours = float(dev_est[w1_mask].sum())\n",
    "        w2_hours = float(dev_est[w2_mask].sum())\n",
    "\n",
    "        def build_details(mask):\n",
    "            dets = []\n",
    "            for idx in dev_df.loc[mask].index:\n",
    "                dets.append({\n",
    "                    \"key\": str(dev_df.loc[idx, key_col]) if key_col else None,\n",
    "                    \"planned_hours\": float(estimate_series.loc[idx]),\n",
    "                    \"tipo\": str(dev_df.loc[idx, tipo_col]) if tipo_col else None,\n",
    "                    \"due\": (delivery_series.loc[idx].strftime(\"%Y-%m-%d\") if pd.notna(delivery_series.loc[idx]) else None),\n",
    "                })\n",
    "            return dets\n",
    "\n",
    "        # issues (solo keys) por compatibilidad con scripts previos\n",
    "        w1_issues = []\n",
    "        w2_issues = []\n",
    "        if key_col:\n",
    "            w1_issues = dev_df.loc[w1_mask, key_col].astype(str).tolist()\n",
    "            w2_issues = dev_df.loc[w2_mask, key_col].astype(str).tolist()\n",
    "\n",
    "        out_by_dev[str(dev)] = {\n",
    "            \"sprint_inicio\": sprint_start.strftime(\"%Y-%m-%d\"),\n",
    "            \"sprint_fin\":    sprint_end.strftime(\"%Y-%m-%d\"),\n",
    "            \"datos_semana_1\": {\n",
    "                \"inicio\": windows[\"week1\"][0].strftime(\"%Y-%m-%d\"),\n",
    "                \"fin\":    windows[\"week1\"][1].strftime(\"%Y-%m-%d\"),\n",
    "                \"tarjetas_planificadas\": w1_cards,\n",
    "                \"horas_planificadas\": w1_hours,\n",
    "                \"issues\": w1_issues,\n",
    "                \"issues_detail\": build_details(w1_mask),\n",
    "            },\n",
    "            \"datos_semana_2\": {\n",
    "                \"inicio\": windows[\"week2\"][0].strftime(\"%Y-%m-%d\"),\n",
    "                \"fin\":    windows[\"week2\"][1].strftime(\"%Y-%m-%d\"),\n",
    "                \"tarjetas_planificadas\": w2_cards,\n",
    "                \"horas_planificadas\": w2_hours,\n",
    "                \"issues\": w2_issues,\n",
    "                \"issues_detail\": build_details(w2_mask),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        totals_w1_cards += w1_cards\n",
    "        totals_w1_hours += w1_hours\n",
    "        totals_w2_cards += w2_cards\n",
    "        totals_w2_hours += w2_hours\n",
    "\n",
    "    result = {\n",
    "        \"sprint_id\": sprint_id,\n",
    "        \"generado_en\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"sprint_inicio_global\": min(global_starts).strftime(\"%Y-%m-%d\"),\n",
    "        \"sprint_fin_global\":    max(global_ends).strftime(\"%Y-%m-%d\"),\n",
    "        \"datos_semana_1\": {\n",
    "            \"tarjetas_planificadas\": int(totals_w1_cards),\n",
    "            \"horas_planificadas\": float(totals_w1_hours),\n",
    "        },\n",
    "        \"datos_semana_2\": {\n",
    "            \"tarjetas_planificadas\": int(totals_w2_cards),\n",
    "            \"horas_planificadas\": float(totals_w2_hours),\n",
    "        },\n",
    "        \"detalle_por_desarrollador\": out_by_dev,\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_json_store(path: str, sprint_id: str, payload: Dict) -> None:\n",
    "    store = {}\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                store = json.load(f)\n",
    "        except Exception:\n",
    "            store = {}\n",
    "    store[sprint_id] = payload\n",
    "    tmp_path = path + \".tmp\"\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(store, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    today = datetime.now()\n",
    "    df = fetch_sheet_with_service_account_by_gid(\n",
    "        sheet_id=SHEET_ID,\n",
    "        worksheet_gid=WORKSHEET_GID,\n",
    "        creds_path=CREDS_PATH,\n",
    "        header_row=3,\n",
    "        first_data_row=4,\n",
    "    )\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"La hoja está vacía o no se pudo leer.\")\n",
    "\n",
    "    df = strip_sheet_gaps(df)\n",
    "\n",
    "    # Normalizar nombre Sprint por si trae espacios\n",
    "    for c in list(df.columns):\n",
    "        if c.strip().lower() == \"sprint\" and c != SPRINT_COL:\n",
    "            df.rename(columns={c: SPRINT_COL}, inplace=True)\n",
    "\n",
    "    metrics = compute_metrics_for_sprint(df, today=today)\n",
    "    update_json_store(OUTPUT_JSON, metrics[\"sprint_id\"], metrics)\n",
    "\n",
    "    print(f\"[OK] Actualizado {OUTPUT_JSON} para sprint {metrics['sprint_id']}\")\n",
    "    print(\"Semana 1:\", metrics[\"datos_semana_1\"])\n",
    "    print(\"Semana 2:\", metrics[\"datos_semana_2\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc5389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️  Sprint: S20251027 | Ventana: 2025-10-27 → 2025-11-14 (incl.)\n",
      "🧾 Issues a consultar: 34\n",
      "[OK] Reporte escrito en report_it.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Lee bt_it_sprint_planning.json, consulta Jira por estado actual y worklogs en la\n",
    "ventana del sprint (global [min(inicio), max(fin)]) y guarda report_it.json.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pytz\n",
    "\n",
    "PLANNING_JSON = os.getenv(\"PLANNING_JSON\", \"bt_it_sprint_planning.json\")\n",
    "REPORT_JSON   = os.getenv(\"REPORT_JSON\",   \"report_it.json\")\n",
    "\n",
    "\n",
    "JIRA_API_ROOT = os.getenv(\"JIRA_API_ROOT\", \"https://team-1583163151751.atlassian.net/rest/api/3\")\n",
    "SEARCH_JQL_URL = f\"{JIRA_API_ROOT}/search/jql\"\n",
    "\n",
    "REAL_HOURS_CF = \"customfield_10046\"\n",
    "# Campos mínimos\n",
    "FIELDS = [\n",
    "    \"summary\", \"status\", \"statuscategorychangedate\", \"assignee\", \"priority\",\n",
    "    \"timetracking\", \"duedate\", \"updated\", \"resolutiondate\" , REAL_HOURS_CF\n",
    "]\n",
    "\n",
    "MAX_RESULTS  = 100\n",
    "CHUNK_ISSUES = 100\n",
    "TIMEOUTS     = (5, 30)\n",
    "TZ = pytz.timezone(\"America/Argentina/Buenos_Aires\")\n",
    "\n",
    "DEV_MAP = {\n",
    "    # \"Alan Mori - Carestino\": \"Alan Mori - Carestino\",\n",
    "    # Mapear si los displayName de Jira difieren de los del Sheet\n",
    "}\n",
    "\n",
    "def load_planning(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path):\n",
    "        raise SystemExit(f\"No existe {path}. Ejecutá primero collect_planned_metrics.py\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def parse_sprint_date(sprint_id: str) -> datetime:\n",
    "    m = re.match(r\"^S(\\d{8})$\", sprint_id)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Sprint inválido: {sprint_id}\")\n",
    "    return datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "\n",
    "def pick_latest_sprint_key(store: Dict[str, Any]) -> str:\n",
    "    keys = [k for k in store.keys() if re.match(r\"^S\\d{8}$\", k)]\n",
    "    if not keys:\n",
    "        raise SystemExit(\"No encontré sprints en planning JSON.\")\n",
    "    keys.sort(key=lambda k: parse_sprint_date(k))\n",
    "    return keys[-1]\n",
    "\n",
    "def extract_issue_keys(planning: Dict[str, Any], sprint_id: str) -> List[str]:\n",
    "    bucket = planning.get(sprint_id, {})\n",
    "    detail = bucket.get(\"detalle_por_desarrollador\", {}) or {}\n",
    "    keys = []\n",
    "    for dev, dev_data in detail.items():\n",
    "        for wlabel in (\"datos_semana_1\", \"datos_semana_2\"):\n",
    "            w = dev_data.get(wlabel, {}) or {}\n",
    "            # preferir issues_detail si está\n",
    "            if \"issues_detail\" in w and isinstance(w[\"issues_detail\"], list):\n",
    "                for it in w[\"issues_detail\"]:\n",
    "                    k = (it or {}).get(\"key\")\n",
    "                    if k: keys.append(str(k).strip())\n",
    "            else:\n",
    "                for k in w.get(\"issues\", []) or []:\n",
    "                    keys.append(str(k).strip())\n",
    "    # unique keep order\n",
    "    seen, out = set(), []\n",
    "    for k in keys:\n",
    "        if k not in seen:\n",
    "            seen.add(k); out.append(k)\n",
    "    return out\n",
    "\n",
    "def fetch_issues_status(issue_keys: List[str]) -> Dict[str, Dict[str, Any]]:\n",
    "    if not EMAIL or not API_TOKEN:\n",
    "        raise SystemExit(\"Faltan credenciales JIRA_EMAIL / JIRA_API_TOKEN.\")\n",
    "\n",
    "    auth = HTTPBasicAuth(EMAIL, API_TOKEN)\n",
    "    headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    result: Dict[str, Dict[str, Any]] = {}\n",
    "    for i in range(0, len(issue_keys), CHUNK_ISSUES):\n",
    "        slice_keys = issue_keys[i:i+CHUNK_ISSUES]\n",
    "        jql = f\"issuekey in ({','.join(slice_keys)})\"\n",
    "\n",
    "        next_token = None\n",
    "        while True:\n",
    "            payload = {\"jql\": jql, \"fields\": FIELDS, \"maxResults\": MAX_RESULTS}\n",
    "            if next_token: payload[\"nextPageToken\"] = next_token\n",
    "\n",
    "            r = requests.post(SEARCH_JQL_URL, auth=auth, headers=headers, json=payload, timeout=TIMEOUTS)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            for issue in data.get(\"issues\", []) or []:\n",
    "                key = issue.get(\"key\")\n",
    "                f = issue.get(\"fields\", {}) or {}\n",
    "                status_obj = f.get(\"status\") or {}\n",
    "                assignee_obj = f.get(\"assignee\") or {}\n",
    "                result[key] = {\n",
    "                    \"summary\": f.get(\"summary\"),\n",
    "                    \"status\": status_obj.get(\"name\"),\n",
    "                    \"status_category\": (status_obj.get(\"statusCategory\") or {}).get(\"name\"),\n",
    "                    \"assignee\": assignee_obj.get(\"displayName\"),\n",
    "                    \"priority\": (f.get(\"priority\") or {}).get(\"name\"),\n",
    "                    \"duedate\": f.get(\"duedate\"),\n",
    "                    \"updated\": f.get(\"updated\"),\n",
    "                    \"resolutiondate\": f.get(\"resolutiondate\"),\n",
    "                    REAL_HOURS_CF: f.get(REAL_HOURS_CF),\n",
    "                }\n",
    "\n",
    "            if data.get(\"isLast\", False): break\n",
    "            next_token = data.get(\"nextPageToken\")\n",
    "            if not next_token: break\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    return result\n",
    "\n",
    "def fetch_worklogs_window(keys: List[str], start_dt: datetime, end_dt: datetime,\n",
    "                          jira_map: Dict[str, Dict[str, Any]] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Trae worklogs dentro de la ventana [start_dt, end_dt] únicamente del usuario asignado a la tarjeta.\n",
    "    \"\"\"\n",
    "    auth = HTTPBasicAuth(EMAIL, API_TOKEN)\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "    # Normalizar fechas con tz\n",
    "    if start_dt.tzinfo is None:\n",
    "        start_dt = TZ.localize(start_dt)\n",
    "    else:\n",
    "        start_dt = start_dt.astimezone(TZ)\n",
    "    if end_dt.tzinfo is None:\n",
    "        end_dt = TZ.localize(end_dt)\n",
    "    else:\n",
    "        end_dt = end_dt.astimezone(TZ)\n",
    "\n",
    "    out = {}\n",
    "    for key in keys:\n",
    "        # Tomar el assignee si se tiene del mapa de issues\n",
    "        assignee_name = None\n",
    "        if jira_map and key in jira_map:\n",
    "            assignee_name = jira_map[key].get(\"assignee\")\n",
    "\n",
    "        url = f\"{JIRA_API_ROOT}/issue/{key}/worklog\"\n",
    "        startAt = 0\n",
    "        total_sec = 0\n",
    "        by_author = {}\n",
    "\n",
    "        while True:\n",
    "            r = requests.get(url, auth=auth, headers=headers, params={\"startAt\": startAt, \"maxResults\": 100}, timeout=TIMEOUTS)\n",
    "            if r.status_code == 404:\n",
    "                # Issue inexistente o sin permiso\n",
    "                print(f\"⚠️ No se pudo leer worklogs de {key} (404)\")\n",
    "                break\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            for wl in data.get(\"worklogs\", []) or []:\n",
    "                started = wl.get(\"started\")\n",
    "                if not started:\n",
    "                    continue\n",
    "                dt = datetime.strptime(started, \"%Y-%m-%dT%H:%M:%S.%f%z\").astimezone(TZ)\n",
    "                if not (start_dt <= dt <= end_dt):\n",
    "                    continue\n",
    "\n",
    "                author = (wl.get(\"author\") or {}).get(\"displayName\") or \"Desconocido\"\n",
    "\n",
    "                # Si hay un assignee definido, filtramos solo sus worklogs\n",
    "                if assignee_name and author != assignee_name:\n",
    "                    continue\n",
    "\n",
    "                sec = int(wl.get(\"timeSpentSeconds\") or 0)\n",
    "                total_sec += sec\n",
    "                by_author[author] = by_author.get(author, 0) + sec\n",
    "\n",
    "            startAt += data.get(\"maxResults\", 0)\n",
    "            if startAt >= data.get(\"total\", 0):\n",
    "                break\n",
    "\n",
    "        out[key] = {\"total_sec\": total_sec, \"by_author\": by_author}\n",
    "\n",
    "    return out\n",
    "\n",
    "def main(sprint_id: str = None):\n",
    "    planning = load_planning(PLANNING_JSON)\n",
    "    if sprint_id is None:\n",
    "        sprint_id = pick_latest_sprint_key(planning)\n",
    "\n",
    "    bucket = planning[sprint_id]\n",
    "    s_start = datetime.strptime(bucket[\"sprint_inicio_global\"], \"%Y-%m-%d\").replace(hour=0, minute=0, second=0)\n",
    "    s_end   = datetime.strptime(bucket[\"sprint_fin_global\"],    \"%Y-%m-%d\").replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    print(f\"➡️  Sprint: {sprint_id} | Ventana: {s_start.date()} → {s_end.date()} (incl.)\")\n",
    "\n",
    "    keys = extract_issue_keys(planning, sprint_id)\n",
    "    if not keys:\n",
    "        raise SystemExit(\"No hay issues en planificación.\")\n",
    "    print(f\"🧾 Issues a consultar: {len(keys)}\")\n",
    "\n",
    "    jira_map = fetch_issues_status(keys)\n",
    "    valid_keys = [k for k in keys if isinstance(k, str) and \"-\" in k and not k.startswith(\"S\")]\n",
    "    worklogs = fetch_worklogs_window(valid_keys, s_start, s_end, jira_map=jira_map)\n",
    "\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    issues_out = {}\n",
    "    resumen_por_estado = defaultdict(int)\n",
    "    resumen_por_dev    = defaultdict(int)\n",
    "\n",
    "    # Aux: mapa dev por issue desde planning\n",
    "    dev_map_issue = {}\n",
    "    for dev, dev_data in bucket[\"detalle_por_desarrollador\"].items():\n",
    "        for w in (\"datos_semana_1\", \"datos_semana_2\"):\n",
    "            detail = dev_data[w].get(\"issues_detail\", []) or []\n",
    "            for it in detail:\n",
    "                if not it: continue\n",
    "                k = it.get(\"key\") \n",
    "                if not k: continue\n",
    "                dev_map_issue[str(k)] = dev\n",
    "\n",
    "            if not detail and dev_data[w].get(\"issues\"):\n",
    "                for k in dev_data[w][\"issues\"]:\n",
    "                    dev_map_issue[str(k)] = dev\n",
    "\n",
    "    for k in keys:\n",
    "        j = jira_map.get(k, {})\n",
    "        dev = dev_map_issue.get(k)\n",
    "        status = j.get(\"status\", \"DESCONOCIDO\")\n",
    "        wl = worklogs.get(k, {\"total_sec\": 0, \"by_author\": {}})\n",
    "\n",
    "        # NUEVO: horas reales por tarjeta desde el custom (entero = horas)\n",
    "        raw_cf = j.get(REAL_HOURS_CF)\n",
    "        hours_cf = 0.0\n",
    "        if isinstance(raw_cf, (int, float)):\n",
    "            hours_cf = float(raw_cf)\n",
    "        elif isinstance(raw_cf, str) and raw_cf.strip().replace('.', '', 1).isdigit():\n",
    "            hours_cf = float(raw_cf.strip())\n",
    "\n",
    "        hours_wl = (wl.get(\"total_sec\", 0) or 0) / 3600.0\n",
    "\n",
    "        if hours_cf > 0:\n",
    "            real_hours_issue = hours_cf\n",
    "            real_source = REAL_HOURS_CF\n",
    "        else:\n",
    "            real_hours_issue = hours_wl\n",
    "            real_source = \"worklogs_window\"\n",
    "\n",
    "        DONE_STATUSES = {\n",
    "            \"Done\",\n",
    "            \"FINISH\",\n",
    "            \"TO DEPLOY\",\n",
    "            \"PENDING REVIEW\",\n",
    "            \"REVIEWING\",\n",
    "            \"TO DEPLOY QA\",\n",
    "        }\n",
    "\n",
    "        status_name = (j.get(\"status\") or \"\").strip()\n",
    "        status_cat = (j.get(\"status_category\") or \"\").strip()\n",
    "        issues_out[k] = {\n",
    "            \"developer\": dev,\n",
    "            \"summary\": j.get(\"summary\"),\n",
    "            \"status\": status,\n",
    "            \"status_category\": j.get(\"status_category\"),\n",
    "            \"assignee\": j.get(\"assignee\"),\n",
    "            \"priority\": j.get(\"priority\"),\n",
    "            \"duedate\": j.get(\"duedate\"),\n",
    "            \"updated\": j.get(\"updated\"),\n",
    "            \"resolutiondate\": j.get(\"resolutiondate\"),\n",
    "            \"worklogs\": wl,\n",
    "            \"customfield_10046_raw\": raw_cf,      # <-- NUEVO\n",
    "            \"real_hours_issue\": real_hours_issue, # <-- NUEVO\n",
    "            \"real_hours_source\": real_source,     # <-- NUEVO\n",
    "            \"done\": (status_cat == \"Done\") or (status_name in DONE_STATUSES),\n",
    "            \"done_before_due\": False,  # se setea abajo si aplica\n",
    "            \"capturado_en\": now,\n",
    "        }\n",
    "\n",
    "        # done_before_due\n",
    "        try:\n",
    "            if j.get(\"resolutiondate\") and j.get(\"duedate\"):\n",
    "                res = datetime.fromisoformat(j[\"resolutiondate\"].replace(\"Z\", \"+00:00\")).astimezone(TZ)\n",
    "                due = datetime.strptime(j[\"duedate\"], \"%Y-%m-%d\").replace(tzinfo=TZ, hour=23, minute=59, second=59)\n",
    "                issues_out[k][\"done_before_due\"] = res <= due\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        resumen_por_estado[status] += 1\n",
    "        if dev: resumen_por_dev[dev] += 1\n",
    "\n",
    "    payload = {\n",
    "        \"generado_en\": now,\n",
    "        \"sprint_id\": sprint_id,\n",
    "        \"sprint_inicio_global\": bucket[\"sprint_inicio_global\"],\n",
    "        \"sprint_fin_global\": bucket[\"sprint_fin_global\"],\n",
    "        \"issues\": issues_out,\n",
    "        \"resumen\": {\n",
    "            \"por_estado\": dict(resumen_por_estado),\n",
    "            \"por_desarrollador\": dict(resumen_por_dev),\n",
    "            \"total\": len(keys),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    store = {}\n",
    "    if os.path.exists(REPORT_JSON):\n",
    "        try:\n",
    "            with open(REPORT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "                store = json.load(f)\n",
    "        except Exception:\n",
    "            store = {}\n",
    "    store[sprint_id] = payload\n",
    "\n",
    "    tmp = REPORT_JSON + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(store, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, REPORT_JSON)\n",
    "\n",
    "    print(f\"[OK] Reporte escrito en {REPORT_JSON}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(SPRINT_A_CALCULAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63fb0beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] KPIs escritos en kpis_it.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Cruza bt_it_sprint_planning.json + report_it.json y genera KPIs:\n",
    "- % tarjetas completadas\n",
    "- % horas cumplidas\n",
    "- Desviación promedio de estimación\n",
    "- % tareas replanificadas\n",
    "- Carga planificada por dev\n",
    "- Predictibilidad (% terminadas antes del due date)\n",
    "Salida: kpis_it.json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "PLANNING=\"bt_it_sprint_planning.json\"\n",
    "REPORT=\"report_it.json\"\n",
    "OUT=\"kpis_it.json\"\n",
    "\n",
    "DEV_MAP = {\n",
    "    # Opcional: mapear nombres del Sheet -> displayName de Jira\n",
    "    # \"Alan Mori - Carestino\": \"Alan Mori - Carestino\",\n",
    "}\n",
    "\n",
    "def parse_sprint_date(sprint_id: str) -> datetime:\n",
    "    m = re.match(r\"^S(\\d{8})$\", sprint_id)\n",
    "    return datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "\n",
    "def pick_latest_sprint_key(store: Dict[str, Any]) -> str:\n",
    "    keys = [k for k in store.keys() if re.match(r\"^S\\d{8}$\", k)]\n",
    "    keys.sort(key=lambda k: parse_sprint_date(k))\n",
    "    return keys[-1]\n",
    "\n",
    "def loadj(p):\n",
    "    if not os.path.exists(p): raise SystemExit(f\"No existe {p}\")\n",
    "    with open(p,\"r\",encoding=\"utf-8\") as f: return json.load(f)\n",
    "\n",
    "def main(sprint_id=None):\n",
    "    plan = loadj(PLANNING)\n",
    "    if sprint_id is None:\n",
    "        sprint_id = pick_latest_sprint_key(plan)\n",
    "    rep_store = loadj(REPORT)\n",
    "    if sprint_id not in rep_store:\n",
    "        raise SystemExit(f\"No hay reporte para {sprint_id} en {REPORT}\")\n",
    "\n",
    "    bucket = plan[sprint_id]\n",
    "    report = rep_store[sprint_id][\"issues\"]\n",
    "\n",
    "    # --- Acumuladores por dev ---\n",
    "    by_dev = defaultdict(lambda: {\n",
    "        \"planned_cards\":0,\n",
    "        \"planned_hours\":0.0,\n",
    "        \"done_cards\":0,\n",
    "        \"real_hours\":0.0,\n",
    "    })\n",
    "\n",
    "    # map “planned_hours por issue”\n",
    "    per_issue_planned = {}  # key -> planned_hours\n",
    "    # y dev por issue según planificación\n",
    "    dev_by_issue = {}\n",
    "\n",
    "    detail = bucket[\"detalle_por_desarrollador\"]\n",
    "    for dev, d in detail.items():\n",
    "        for w in (\"datos_semana_1\",\"datos_semana_2\"):\n",
    "            dd = d.get(w, {})\n",
    "            by_dev[dev][\"planned_cards\"] += len(dd.get(\"issues\", []))\n",
    "            by_dev[dev][\"planned_hours\"] += float(dd.get(\"horas_planificadas\", 0))\n",
    "            for it in dd.get(\"issues_detail\", []) or []:\n",
    "                k = (it or {}).get(\"key\")\n",
    "                if not k: continue\n",
    "                per_issue_planned[k] = float((it.get(\"planned_hours\") or 0))\n",
    "                dev_by_issue[k] = dev\n",
    "            # compat: si no hay issues_detail, usar lista simple\n",
    "            if not dd.get(\"issues_detail\") and dd.get(\"issues\"):\n",
    "                for k in dd.get(\"issues\"):\n",
    "                    dev_by_issue[k] = dev\n",
    "\n",
    "    # --- Sumar realizado por dev ---\n",
    "    for k, info in report.items():\n",
    "        dev = dev_by_issue.get(k) or info.get(\"developer\")\n",
    "        if not dev:\n",
    "            continue\n",
    "        if info.get(\"done\"):\n",
    "            by_dev[dev][\"done_cards\"] += 1\n",
    "\n",
    "        # Preferir horas reales del custom; fallback: worklogs\n",
    "        wl = info.get(\"worklogs\", {})\n",
    "        if isinstance(wl.get(\"by_author\"), dict) and any(wl[\"by_author\"].values()):\n",
    "            author_name = DEV_MAP.get(dev, dev)\n",
    "            real_hours = int(wl[\"by_author\"].get(author_name, 0)) / 3600.0\n",
    "        else:\n",
    "            # fallback si no hay worklogs (tarjeta sin registrar tiempo)\n",
    "            if isinstance(info.get(\"real_hours_issue\"), (int, float)) and info[\"real_hours_issue\"] > 0:\n",
    "                real_hours = float(info[\"real_hours_issue\"])\n",
    "            else:\n",
    "                real_hours = 0.0\n",
    "\n",
    "        by_dev[dev][\"real_hours\"] += real_hours\n",
    "\n",
    "    # --- KPIs por dev + desviación promedio ---\n",
    "    kpis_dev = {}\n",
    "    for dev, v in by_dev.items():\n",
    "        pc, ph, dc, rh = v[\"planned_cards\"], v[\"planned_hours\"], v[\"done_cards\"], v[\"real_hours\"]\n",
    "        pct_cards = (dc/pc*100) if pc else None\n",
    "        pct_hours = (rh/ph*100) if ph else None\n",
    "\n",
    "        # desviación promedio por issue planificada del dev\n",
    "        dev_issues = [k for k, d in dev_by_issue.items() if d == dev]\n",
    "        dev_devs = []\n",
    "        for k in dev_issues:\n",
    "            p = per_issue_planned.get(k, 0.0)\n",
    "            if p <= 0:\n",
    "                continue\n",
    "\n",
    "            # ✅ Nuevo: preferir horas reales del customfield_10046 si existe\n",
    "            info_k = report.get(k, {})\n",
    "            if isinstance(info_k.get(\"real_hours_issue\"), (int, float)) and info_k[\"real_hours_issue\"] > 0:\n",
    "                real_issue = float(info_k[\"real_hours_issue\"])\n",
    "            else:\n",
    "                wl = info_k.get(\"worklogs\", {})\n",
    "                if isinstance(wl.get(\"by_author\"), dict):\n",
    "                    author_name = DEV_MAP.get(dev, dev)\n",
    "                    real_issue_sec = int(wl[\"by_author\"].get(author_name, 0))\n",
    "                else:\n",
    "                    real_issue_sec = int(wl.get(\"total_sec\", 0))\n",
    "                real_issue = real_issue_sec / 3600.0\n",
    "\n",
    "            dev_devs.append(abs(p - real_issue) / p)\n",
    "\n",
    "        avg_dev = (sum(dev_devs)/len(dev_devs)) if dev_devs else None\n",
    "\n",
    "        kpis_dev[dev] = {\n",
    "            \"tarjetas_planificadas\": pc,\n",
    "            \"tarjetas_realizadas\": dc,\n",
    "            \"porc_tarjetas_completadas\": round(pct_cards,1) if pct_cards is not None else None,\n",
    "            \"horas_planificadas\": round(ph,1),\n",
    "            \"horas_realizadas\": round(rh,1),\n",
    "            \"porc_horas_cumplidas\": round(pct_hours,1) if pct_hours is not None else None,\n",
    "            \"desviacion_promedio_estimacion\": round(avg_dev,3) if avg_dev is not None else None,\n",
    "        }\n",
    "\n",
    "    # --- Globales ---\n",
    "    all_keys = list(report.keys())\n",
    "    done_keys = [k for k, i in report.items() if i.get(\"done\")]\n",
    "    replan_pct = round((1 - len(done_keys)/len(all_keys))*100,1) if all_keys else None\n",
    "\n",
    "    completed_with_due = [k for k in done_keys if report[k].get(\"duedate\")]\n",
    "    ontime = [k for k in completed_with_due if report[k].get(\"done_before_due\")]\n",
    "    predict = round(len(ontime)/len(completed_with_due)*100,1) if completed_with_due else None\n",
    "\n",
    "    out = {\n",
    "        \"sprint_id\": sprint_id,\n",
    "        \"kpis_por_desarrollador\": kpis_dev,\n",
    "        \"kpis_globales\": {\n",
    "            \"porc_tareas_replanificadas\": replan_pct,\n",
    "            \"predictibilidad_por_due_date\": predict,\n",
    "            \"carga_planificada_por_dev\": {d: kpis_dev[d][\"horas_planificadas\"] for d in kpis_dev},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    store = {}\n",
    "    if os.path.exists(OUT):\n",
    "        try:\n",
    "            with open(OUT, \"r\", encoding=\"utf-8\") as f:\n",
    "                store = json.load(f)\n",
    "        except Exception:\n",
    "            store = {}\n",
    "    store[sprint_id] = out\n",
    "\n",
    "    tmp = OUT + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(store, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, OUT)\n",
    "\n",
    "    print(f\"[OK] KPIs escritos en {OUT}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main(SPRINT_A_CALCULAR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
