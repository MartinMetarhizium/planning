{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a54de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import JQL, FIELDS, BASE_URL, JIRA_DOMAIN,EMAIL, MAX_RESULTS, MODULE_DEVS, VALID_STATUSES, MAIL_MAP, DAILY_HOURS,MIN_PROJECT_RATIO,PROJECT_MAP, DEFAULT_END_DATE, DEFAULT_END_DATE_with_timezone, DEFAULT_START_DATE,DEFAULT_START_DATE_with_timezone\n",
    "from token_hidden import API_TOKEN\n",
    "SPRINT_A_CALCULAR = \"S20251027\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13c83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Actualizado ./bt_it_sprint_planning.json para sprint S20251027\n",
      "Semana 1: {'tarjetas_planificadas': 15, 'horas_planificadas': 126.0}\n",
      "Semana 2: {'tarjetas_planificadas': 20, 'horas_planificadas': 200.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Temp\\ipykernel_25436\\1907969500.py:111: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Recolector de planificaci√≥n (IT/BT) desde Google Sheets.\n",
    "\n",
    "- Lee la hoja (ID + GID).\n",
    "- Detecta el sprint actual (col 'Sprint' o escaneando SYYYYMMDD).\n",
    "- Por desarrollador:\n",
    "    * Inicio real de sprint: min('Fecha inicio sprint') o fallback por c√≥digo de sprint.\n",
    "    * Fin real de sprint: max('Fecha fin sprint') o fallback = inicio + 11 d√≠as (23:59:59).\n",
    "    * Filtra tarjetas con 'Fecha Estimada' (columna AQ o header) ‚àà [inicio, fin].\n",
    "    * Separa en week1 (lun‚Äìvie primera) y week2 (lun segunda ‚Üí fin real).\n",
    "    * Suma tarjetas y horas planificadas y guarda detalle por issue.\n",
    "\n",
    "Salida: bt_it_sprint_planning.json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =======================\n",
    "# ===== CONFIG INICIO ===\n",
    "# =======================\n",
    "\n",
    "SHEET_ID = \"1NW5uT16Req72uMfYZ6-eITvwk9uKvXncEYsqWou9-D0\"\n",
    "WORKSHEET_GID = 985465980\n",
    "CREDS_PATH = \"./service_account.json\"\n",
    "OUTPUT_JSON = \"./bt_it_sprint_planning.json\"\n",
    "\n",
    "# Fecha de entrega:\n",
    "DELIVERY_DATE_HEADER = \"Fecha Estimada\"\n",
    "FORCE_COLUMN_LETTER = \"AQ\"   # forzar AQ como ‚ÄúFecha Estimada‚Äù\n",
    "\n",
    "# Columnas para horas planificadas (primera que exista)\n",
    "ESTIMATE_CANDIDATES = [\n",
    "    \"Estimacion\", \"Estimaci√≥n\", \"Estimaci√≥n Personal\", \"Story point estimate\"\n",
    "]\n",
    "\n",
    "SPRINT_COL = \"Sprint\"\n",
    "DEV_COL_CANDIDATES = [\"Desarrollador\", \"Persona asignada\"]\n",
    "SPRINT_START_HEADER = \"Fecha inicio sprint\"\n",
    "CARD_START_HEADER   = \"Fecha inicio tarjeta\"\n",
    "SPRINT_END_HEADER   = \"Fecha fin sprint\"\n",
    "KEY_COL_CANDIDATES  = [\"Clave\", \"ID\"]  # ‚ÄúClave‚Äù en tu Sheet\n",
    "TYPE_COL            = \"Tipo\"\n",
    "\n",
    "# =======================\n",
    "# ===== CONFIG FIN ======\n",
    "# =======================\n",
    "\n",
    "\n",
    "# -------- Google Sheets helpers --------\n",
    "def fetch_sheet_with_service_account_by_gid(\n",
    "    sheet_id: str,\n",
    "    worksheet_gid: int,\n",
    "    creds_path: str,\n",
    "    header_row: int = 3,      # header en fila 3\n",
    "    first_data_row: int = 4,  # datos a partir de fila 4 (puede haber huecos)\n",
    ") -> pd.DataFrame:\n",
    "    import gspread\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "    scope = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "        \"https://www.googleapis.com/auth/drive.readonly\",\n",
    "    ]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(creds_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    sh = client.open_by_key(sheet_id)\n",
    "    ws = sh.get_worksheet_by_id(worksheet_gid)\n",
    "    if ws is None:\n",
    "        raise RuntimeError(f\"No encontr√© worksheet con gid={worksheet_gid}\")\n",
    "\n",
    "    values = ws.get_all_values()\n",
    "    if not values or len(values) < header_row:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    raw_headers = values[header_row - 1]\n",
    "    rows = values[first_data_row - 1:]\n",
    "\n",
    "    def norm(h):\n",
    "        return (h or \"\").replace(\"\\u00a0\", \" \").strip()\n",
    "\n",
    "    headers, seen = [], {}\n",
    "    for idx, h in enumerate(raw_headers, start=1):\n",
    "        name = norm(h) or f\"__blank_{idx}\"\n",
    "        base = name\n",
    "        n = seen.get(base, 0)\n",
    "        if n:\n",
    "            name = f\"{base}_{n+1}\"\n",
    "        seen[base] = n + 1\n",
    "        headers.append(name)\n",
    "\n",
    "    norm_rows = [\n",
    "        r + [\"\"] * (len(headers) - len(r)) if len(r) < len(headers) else r[:len(headers)]\n",
    "        for r in rows\n",
    "    ]\n",
    "    df = pd.DataFrame(norm_rows, columns=headers).dropna(axis=1, how=\"all\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def strip_sheet_gaps(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df = df.replace(\"\\u00a0\", \" \", regex=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    empty_all = df.isna().all(axis=1) | df.apply(lambda r: all(str(x).strip() == \"\" for x in r), axis=1)\n",
    "    df = df.loc[~empty_all].copy()\n",
    "\n",
    "    if SPRINT_COL in df.columns:\n",
    "        s = df[SPRINT_COL].astype(str).str.strip()\n",
    "        if (s != \"\").any():\n",
    "            first_idx = s.ne(\"\").idxmax()\n",
    "            df = df.loc[first_idx:].copy()\n",
    "\n",
    "    empty_all = df.isna().all(axis=1) | df.apply(lambda r: all(str(x).strip() == \"\" for x in r), axis=1)\n",
    "    df = df.loc[~empty_all].copy()\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------- General helpers --------\n",
    "def column_letter_to_index(letter: str) -> int:\n",
    "    letter = letter.strip().upper()\n",
    "    result = 0\n",
    "    for ch in letter:\n",
    "        result = result * 26 + (ord(ch) - ord('A') + 1)\n",
    "    return result - 1\n",
    "\n",
    "def parse_date_series(series_like, dayfirst=True) -> pd.Series:\n",
    "    s = pd.Series(series_like)\n",
    "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=dayfirst)\n",
    "\n",
    "def to_float_series(series_like) -> pd.Series:\n",
    "    s = pd.Series(series_like)\n",
    "    def _coerce(x):\n",
    "        if x is None:\n",
    "            return 0.0\n",
    "        sx = str(x).strip().replace(\",\", \".\")\n",
    "        if sx == \"\":\n",
    "            return 0.0\n",
    "        try:\n",
    "            return float(sx)\n",
    "        except:\n",
    "            return 0.0\n",
    "    return s.map(_coerce)\n",
    "\n",
    "def pick_dev_column(df: pd.DataFrame) -> str:\n",
    "    for c in DEV_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if \"asignad\" in c.lower() or \"desarrollador\" in c.lower():\n",
    "            return c\n",
    "    raise RuntimeError(\"No se encontr√≥ columna de desarrollador.\")\n",
    "\n",
    "def pick_key_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in KEY_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if c.strip().lower() in {\"clave\", \"key\", \"issue\", \"ticket\"}:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_current_sprint(df: pd.DataFrame, sprint_col: str, today: datetime) -> Tuple[str, datetime]:\n",
    "    def _pick_best(candidates: List[Tuple[str, datetime]]) -> Tuple[str, datetime]:\n",
    "        le_today = [c for c in candidates if c[1] <= today]\n",
    "        return max(le_today, key=lambda x: x[1]) if le_today else max(candidates, key=lambda x: x[1])\n",
    "\n",
    "    candidates: List[Tuple[str, datetime]] = []\n",
    "\n",
    "    if sprint_col in df.columns:\n",
    "        for val in df[sprint_col].dropna().astype(str).unique():\n",
    "            m = re.match(r\"^S(\\d{8})$\", val.strip())\n",
    "            if m:\n",
    "                d = datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "                candidates.append((val.strip(), d))\n",
    "        if candidates:\n",
    "            return _pick_best(candidates)\n",
    "\n",
    "    for col in df.columns:\n",
    "        for val in df[col].dropna().astype(str).unique():\n",
    "            s = val.strip()\n",
    "            m = re.match(r\"^S(\\d{8})$\", s)\n",
    "            if m:\n",
    "                d = datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "                candidates.append((s, d))\n",
    "    if candidates:\n",
    "        return _pick_best(candidates)\n",
    "\n",
    "    # Derivar desde fecha de inicio (global)\n",
    "    start_candidates = []\n",
    "    if SPRINT_START_HEADER in df.columns:\n",
    "        s = parse_date_series(df[SPRINT_START_HEADER]).dropna()\n",
    "        if not s.empty:\n",
    "            start_candidates.append(s.min())\n",
    "    if not start_candidates and CARD_START_HEADER in df.columns:\n",
    "        s = parse_date_series(df[CARD_START_HEADER]).dropna()\n",
    "        if not s.empty:\n",
    "            start_candidates.append(s.min())\n",
    "    base = start_candidates[0] if start_candidates else today\n",
    "    base = datetime(base.year, base.month, base.day)\n",
    "    sprint_id = f\"S{base.strftime('%Y%m%d')}\"\n",
    "    return sprint_id, base\n",
    "\n",
    "def compute_dev_sprint_start(dev_df: pd.DataFrame, default_start: datetime) -> datetime:\n",
    "    starts = []\n",
    "    if SPRINT_START_HEADER in dev_df.columns:\n",
    "        s = parse_date_series(dev_df[SPRINT_START_HEADER]).dropna()\n",
    "        if not s.empty: starts.append(s.min())\n",
    "    if not starts and CARD_START_HEADER in dev_df.columns:\n",
    "        s = parse_date_series(dev_df[CARD_START_HEADER]).dropna()\n",
    "        if not s.empty: starts.append(s.min())\n",
    "    if not starts:\n",
    "        return default_start\n",
    "    dt = starts[0]\n",
    "    return datetime(dt.year, dt.month, dt.day)\n",
    "\n",
    "def compute_dev_sprint_end(dev_df: pd.DataFrame, default_start: datetime) -> datetime:\n",
    "    if SPRINT_END_HEADER in dev_df.columns:\n",
    "        s = parse_date_series(dev_df[SPRINT_END_HEADER]).dropna()\n",
    "        if not s.empty:\n",
    "            d = s.max().to_pydatetime()\n",
    "            return datetime(d.year, d.month, d.day, 23, 59, 59)\n",
    "    return default_start + timedelta(days=11, hours=23, minutes=59, seconds=59)\n",
    "\n",
    "def choose_delivery_series(df: pd.DataFrame) -> pd.Series:\n",
    "    if DELIVERY_DATE_HEADER in df.columns:\n",
    "        return parse_date_series(df[DELIVERY_DATE_HEADER])\n",
    "    if FORCE_COLUMN_LETTER:\n",
    "        col_idx = column_letter_to_index(FORCE_COLUMN_LETTER)\n",
    "        if col_idx < len(df.columns):\n",
    "            s = df.iloc[:, col_idx]\n",
    "            return parse_date_series(s)\n",
    "        raise RuntimeError(f\"FORCE_COLUMN_LETTER={FORCE_COLUMN_LETTER} excede el ancho de la hoja.\")\n",
    "    raise RuntimeError(\"No encontr√© la columna de fecha de entrega.\")\n",
    "\n",
    "def choose_estimate_series(df: pd.DataFrame) -> pd.Series:\n",
    "    for c in ESTIMATE_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return to_float_series(df[c])\n",
    "    return pd.Series([0.0] * len(df), index=df.index)\n",
    "\n",
    "def within(ts: pd.Timestamp, start: datetime, end: datetime) -> bool:\n",
    "    if pd.isna(ts): return False\n",
    "    d = ts.to_pydatetime()\n",
    "    return start <= d <= end\n",
    "\n",
    "def build_week_windows(sprint_start: datetime, sprint_end: datetime) -> Dict[str, Tuple[datetime, datetime]]:\n",
    "    # Semana 1: lunes ‚Üí viernes\n",
    "    w1_end = sprint_start + timedelta(days=4, hours=23, minutes=59, seconds=59)\n",
    "    # Semana 2: lunes siguiente ‚Üí viernes siguiente\n",
    "    w2_start = sprint_start + timedelta(days=7)\n",
    "    w2_end = sprint_start + timedelta(days=11, hours=23, minutes=59, seconds=59)\n",
    "\n",
    "    # Evitar pasarse del sprint_end real (por si termina antes)\n",
    "    w1_end = min(w1_end, sprint_end)\n",
    "    w2_end = min(w2_end, sprint_end)\n",
    "\n",
    "    return {\n",
    "        \"all\": (sprint_start, sprint_end),\n",
    "        \"week1\": (w1_end - timedelta(days=4), w1_end),\n",
    "        \"week2\": (w2_start, w2_end)\n",
    "    }\n",
    "\n",
    "\n",
    "# -------- Core --------\n",
    "def compute_metrics_for_sprint(df: pd.DataFrame, today: datetime) -> Dict:\n",
    "    sprint_id, sprint_code_date = detect_current_sprint(df, SPRINT_COL, today)\n",
    "    current = df[df[SPRINT_COL].astype(str).str.strip() == sprint_id].copy()\n",
    "    if current.empty:\n",
    "        raise RuntimeError(f\"No hay filas para el sprint {sprint_id}\")\n",
    "\n",
    "    dev_col = pick_dev_column(current)\n",
    "    key_col = pick_key_column(current)\n",
    "    delivery_series = choose_delivery_series(current)\n",
    "    estimate_series = choose_estimate_series(current)\n",
    "    tipo_col = TYPE_COL if TYPE_COL in current.columns else None\n",
    "\n",
    "    out_by_dev: Dict[str, Dict] = {}\n",
    "    totals_w1_cards = totals_w1_hours = 0.0\n",
    "    totals_w2_cards = totals_w2_hours = 0.0\n",
    "    global_starts, global_ends = [], []\n",
    "\n",
    "    for dev, dev_df in current.groupby(dev_col):\n",
    "        dev_df = dev_df.copy()\n",
    "        dev_delivery = delivery_series.loc[dev_df.index]\n",
    "        dev_est = estimate_series.loc[dev_df.index]\n",
    "\n",
    "        sprint_start = compute_dev_sprint_start(dev_df, sprint_code_date)\n",
    "        sprint_end   = compute_dev_sprint_end(dev_df, sprint_start)\n",
    "        windows = build_week_windows(sprint_start, sprint_end)\n",
    "\n",
    "        global_starts.append(sprint_start)\n",
    "        global_ends.append(sprint_end)\n",
    "\n",
    "        w1_mask = dev_delivery.apply(lambda x: within(x, *windows[\"week1\"]))\n",
    "        w2_mask = dev_delivery.apply(lambda x: within(x, *windows[\"week2\"]))\n",
    "\n",
    "        w1_cards = int(w1_mask.sum())\n",
    "        w2_cards = int(w2_mask.sum())\n",
    "\n",
    "        w1_hours = float(dev_est[w1_mask].sum())\n",
    "        w2_hours = float(dev_est[w2_mask].sum())\n",
    "\n",
    "        def build_details(mask):\n",
    "            dets = []\n",
    "            for idx in dev_df.loc[mask].index:\n",
    "                dets.append({\n",
    "                    \"key\": str(dev_df.loc[idx, key_col]) if key_col else None,\n",
    "                    \"planned_hours\": float(estimate_series.loc[idx]),\n",
    "                    \"tipo\": str(dev_df.loc[idx, tipo_col]) if tipo_col else None,\n",
    "                    \"due\": (delivery_series.loc[idx].strftime(\"%Y-%m-%d\") if pd.notna(delivery_series.loc[idx]) else None),\n",
    "                })\n",
    "            return dets\n",
    "\n",
    "        # issues (solo keys) por compatibilidad con scripts previos\n",
    "        w1_issues = []\n",
    "        w2_issues = []\n",
    "        if key_col:\n",
    "            w1_issues = dev_df.loc[w1_mask, key_col].astype(str).tolist()\n",
    "            w2_issues = dev_df.loc[w2_mask, key_col].astype(str).tolist()\n",
    "\n",
    "        out_by_dev[str(dev)] = {\n",
    "            \"sprint_inicio\": sprint_start.strftime(\"%Y-%m-%d\"),\n",
    "            \"sprint_fin\":    sprint_end.strftime(\"%Y-%m-%d\"),\n",
    "            \"datos_semana_1\": {\n",
    "                \"inicio\": windows[\"week1\"][0].strftime(\"%Y-%m-%d\"),\n",
    "                \"fin\":    windows[\"week1\"][1].strftime(\"%Y-%m-%d\"),\n",
    "                \"tarjetas_planificadas\": w1_cards,\n",
    "                \"horas_planificadas\": w1_hours,\n",
    "                \"issues\": w1_issues,\n",
    "                \"issues_detail\": build_details(w1_mask),\n",
    "            },\n",
    "            \"datos_semana_2\": {\n",
    "                \"inicio\": windows[\"week2\"][0].strftime(\"%Y-%m-%d\"),\n",
    "                \"fin\":    windows[\"week2\"][1].strftime(\"%Y-%m-%d\"),\n",
    "                \"tarjetas_planificadas\": w2_cards,\n",
    "                \"horas_planificadas\": w2_hours,\n",
    "                \"issues\": w2_issues,\n",
    "                \"issues_detail\": build_details(w2_mask),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        totals_w1_cards += w1_cards\n",
    "        totals_w1_hours += w1_hours\n",
    "        totals_w2_cards += w2_cards\n",
    "        totals_w2_hours += w2_hours\n",
    "\n",
    "    result = {\n",
    "        \"sprint_id\": sprint_id,\n",
    "        \"generado_en\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"sprint_inicio_global\": min(global_starts).strftime(\"%Y-%m-%d\"),\n",
    "        \"sprint_fin_global\":    max(global_ends).strftime(\"%Y-%m-%d\"),\n",
    "        \"datos_semana_1\": {\n",
    "            \"tarjetas_planificadas\": int(totals_w1_cards),\n",
    "            \"horas_planificadas\": float(totals_w1_hours),\n",
    "        },\n",
    "        \"datos_semana_2\": {\n",
    "            \"tarjetas_planificadas\": int(totals_w2_cards),\n",
    "            \"horas_planificadas\": float(totals_w2_hours),\n",
    "        },\n",
    "        \"detalle_por_desarrollador\": out_by_dev,\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_json_store(path: str, sprint_id: str, payload: Dict) -> None:\n",
    "    store = {}\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                store = json.load(f)\n",
    "        except Exception:\n",
    "            store = {}\n",
    "    store[sprint_id] = payload\n",
    "    tmp_path = path + \".tmp\"\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(store, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    today = datetime.now()\n",
    "    df = fetch_sheet_with_service_account_by_gid(\n",
    "        sheet_id=SHEET_ID,\n",
    "        worksheet_gid=WORKSHEET_GID,\n",
    "        creds_path=CREDS_PATH,\n",
    "        header_row=3,\n",
    "        first_data_row=4,\n",
    "    )\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"La hoja est√° vac√≠a o no se pudo leer.\")\n",
    "\n",
    "    df = strip_sheet_gaps(df)\n",
    "\n",
    "    # Normalizar nombre Sprint por si trae espacios\n",
    "    for c in list(df.columns):\n",
    "        if c.strip().lower() == \"sprint\" and c != SPRINT_COL:\n",
    "            df.rename(columns={c: SPRINT_COL}, inplace=True)\n",
    "\n",
    "    metrics = compute_metrics_for_sprint(df, today=today)\n",
    "    update_json_store(OUTPUT_JSON, metrics[\"sprint_id\"], metrics)\n",
    "\n",
    "    print(f\"[OK] Actualizado {OUTPUT_JSON} para sprint {metrics['sprint_id']}\")\n",
    "    print(\"Semana 1:\", metrics[\"datos_semana_1\"])\n",
    "    print(\"Semana 2:\", metrics[\"datos_semana_2\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc5389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è  Sprint: S20251027 | Ventana: 2025-10-27 ‚Üí 2025-11-14 (incl.)\n",
      "üßæ Issues a consultar: 35\n",
      "[OK] Reporte escrito en report_it.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Lee bt_it_sprint_planning.json, consulta Jira por estado actual y worklogs en la\n",
    "ventana del sprint (global [min(inicio), max(fin)]) y guarda report_it.json.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pytz\n",
    "\n",
    "PLANNING_JSON = os.getenv(\"PLANNING_JSON\", \"bt_it_sprint_planning.json\")\n",
    "REPORT_JSON   = os.getenv(\"REPORT_JSON\",   \"report_it.json\")\n",
    "\n",
    "\n",
    "JIRA_API_ROOT = os.getenv(\"JIRA_API_ROOT\", \"https://team-1583163151751.atlassian.net/rest/api/3\")\n",
    "SEARCH_JQL_URL = f\"{JIRA_API_ROOT}/search/jql\"\n",
    "\n",
    "REAL_HOURS_CF = \"customfield_10046\"\n",
    "# Campos m√≠nimos\n",
    "FIELDS = [\n",
    "    \"summary\", \"status\", \"statuscategorychangedate\", \"assignee\", \"priority\",\n",
    "    \"timetracking\", \"duedate\", \"updated\", \"resolutiondate\" , REAL_HOURS_CF\n",
    "]\n",
    "\n",
    "MAX_RESULTS  = 100\n",
    "CHUNK_ISSUES = 100\n",
    "TIMEOUTS     = (5, 30)\n",
    "TZ = pytz.timezone(\"America/Argentina/Buenos_Aires\")\n",
    "\n",
    "DEV_MAP = {\n",
    "    # \"Alan Mori - Carestino\": \"Alan Mori - Carestino\",\n",
    "    # Mapear si los displayName de Jira difieren de los del Sheet\n",
    "}\n",
    "\n",
    "def load_planning(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path):\n",
    "        raise SystemExit(f\"No existe {path}. Ejecut√° primero collect_planned_metrics.py\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def parse_sprint_date(sprint_id: str) -> datetime:\n",
    "    m = re.match(r\"^S(\\d{8})$\", sprint_id)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Sprint inv√°lido: {sprint_id}\")\n",
    "    return datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "\n",
    "def pick_latest_sprint_key(store: Dict[str, Any]) -> str:\n",
    "    keys = [k for k in store.keys() if re.match(r\"^S\\d{8}$\", k)]\n",
    "    if not keys:\n",
    "        raise SystemExit(\"No encontr√© sprints en planning JSON.\")\n",
    "    keys.sort(key=lambda k: parse_sprint_date(k))\n",
    "    return keys[-1]\n",
    "\n",
    "def extract_issue_keys(planning: Dict[str, Any], sprint_id: str) -> List[str]:\n",
    "    bucket = planning.get(sprint_id, {})\n",
    "    detail = bucket.get(\"detalle_por_desarrollador\", {}) or {}\n",
    "    keys = []\n",
    "    for dev, dev_data in detail.items():\n",
    "        for wlabel in (\"datos_semana_1\", \"datos_semana_2\"):\n",
    "            w = dev_data.get(wlabel, {}) or {}\n",
    "            # preferir issues_detail si est√°\n",
    "            if \"issues_detail\" in w and isinstance(w[\"issues_detail\"], list):\n",
    "                for it in w[\"issues_detail\"]:\n",
    "                    k = (it or {}).get(\"key\")\n",
    "                    if k: keys.append(str(k).strip())\n",
    "            else:\n",
    "                for k in w.get(\"issues\", []) or []:\n",
    "                    keys.append(str(k).strip())\n",
    "    # unique keep order\n",
    "    seen, out = set(), []\n",
    "    for k in keys:\n",
    "        if k not in seen:\n",
    "            seen.add(k); out.append(k)\n",
    "    return out\n",
    "\n",
    "def fetch_issues_status(issue_keys: List[str]) -> Dict[str, Dict[str, Any]]:\n",
    "    if not EMAIL or not API_TOKEN:\n",
    "        raise SystemExit(\"Faltan credenciales JIRA_EMAIL / JIRA_API_TOKEN.\")\n",
    "\n",
    "    auth = HTTPBasicAuth(EMAIL, API_TOKEN)\n",
    "    headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    result: Dict[str, Dict[str, Any]] = {}\n",
    "    for i in range(0, len(issue_keys), CHUNK_ISSUES):\n",
    "        slice_keys = issue_keys[i:i+CHUNK_ISSUES]\n",
    "        jql = f\"issuekey in ({','.join(slice_keys)})\"\n",
    "\n",
    "        next_token = None\n",
    "        while True:\n",
    "            payload = {\"jql\": jql, \"fields\": FIELDS, \"maxResults\": MAX_RESULTS}\n",
    "            if next_token: payload[\"nextPageToken\"] = next_token\n",
    "\n",
    "            r = requests.post(SEARCH_JQL_URL, auth=auth, headers=headers, json=payload, timeout=TIMEOUTS)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            for issue in data.get(\"issues\", []) or []:\n",
    "                key = issue.get(\"key\")\n",
    "                f = issue.get(\"fields\", {}) or {}\n",
    "                status_obj = f.get(\"status\") or {}\n",
    "                assignee_obj = f.get(\"assignee\") or {}\n",
    "                result[key] = {\n",
    "                    \"summary\": f.get(\"summary\"),\n",
    "                    \"status\": status_obj.get(\"name\"),\n",
    "                    \"status_category\": (status_obj.get(\"statusCategory\") or {}).get(\"name\"),\n",
    "                    \"assignee\": assignee_obj.get(\"displayName\"),\n",
    "                    \"priority\": (f.get(\"priority\") or {}).get(\"name\"),\n",
    "                    \"duedate\": f.get(\"duedate\"),\n",
    "                    \"updated\": f.get(\"updated\"),\n",
    "                    \"resolutiondate\": f.get(\"resolutiondate\"),\n",
    "                    REAL_HOURS_CF: f.get(REAL_HOURS_CF),\n",
    "                }\n",
    "\n",
    "            if data.get(\"isLast\", False): break\n",
    "            next_token = data.get(\"nextPageToken\")\n",
    "            if not next_token: break\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    return result\n",
    "\n",
    "def fetch_worklogs_window(keys: List[str], start_dt: datetime, end_dt: datetime,\n",
    "                          jira_map: Dict[str, Dict[str, Any]] = None) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Trae worklogs dentro de la ventana [start_dt, end_dt] √∫nicamente del usuario asignado a la tarjeta.\n",
    "    \"\"\"\n",
    "    auth = HTTPBasicAuth(EMAIL, API_TOKEN)\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "    # Normalizar fechas con tz\n",
    "    if start_dt.tzinfo is None:\n",
    "        start_dt = TZ.localize(start_dt)\n",
    "    else:\n",
    "        start_dt = start_dt.astimezone(TZ)\n",
    "    if end_dt.tzinfo is None:\n",
    "        end_dt = TZ.localize(end_dt)\n",
    "    else:\n",
    "        end_dt = end_dt.astimezone(TZ)\n",
    "\n",
    "    out = {}\n",
    "    for key in keys:\n",
    "        # Tomar el assignee si se tiene del mapa de issues\n",
    "        assignee_name = None\n",
    "        if jira_map and key in jira_map:\n",
    "            assignee_name = jira_map[key].get(\"assignee\")\n",
    "\n",
    "        url = f\"{JIRA_API_ROOT}/issue/{key}/worklog\"\n",
    "        startAt = 0\n",
    "        total_sec = 0\n",
    "        by_author = {}\n",
    "\n",
    "        while True:\n",
    "            r = requests.get(url, auth=auth, headers=headers, params={\"startAt\": startAt, \"maxResults\": 100}, timeout=TIMEOUTS)\n",
    "            if r.status_code == 404:\n",
    "                # Issue inexistente o sin permiso\n",
    "                print(f\"‚ö†Ô∏è No se pudo leer worklogs de {key} (404)\")\n",
    "                break\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "\n",
    "            for wl in data.get(\"worklogs\", []) or []:\n",
    "                started = wl.get(\"started\")\n",
    "                if not started:\n",
    "                    continue\n",
    "                dt = datetime.strptime(started, \"%Y-%m-%dT%H:%M:%S.%f%z\").astimezone(TZ)\n",
    "                if not (start_dt <= dt <= end_dt):\n",
    "                    continue\n",
    "\n",
    "                author = (wl.get(\"author\") or {}).get(\"displayName\") or \"Desconocido\"\n",
    "\n",
    "                # Si hay un assignee definido, filtramos solo sus worklogs\n",
    "                if assignee_name and author != assignee_name:\n",
    "                    continue\n",
    "\n",
    "                sec = int(wl.get(\"timeSpentSeconds\") or 0)\n",
    "                total_sec += sec\n",
    "                by_author[author] = by_author.get(author, 0) + sec\n",
    "\n",
    "            startAt += data.get(\"maxResults\", 0)\n",
    "            if startAt >= data.get(\"total\", 0):\n",
    "                break\n",
    "\n",
    "        out[key] = {\"total_sec\": total_sec, \"by_author\": by_author}\n",
    "\n",
    "    return out\n",
    "\n",
    "def main(sprint_id: str = None):\n",
    "    planning = load_planning(PLANNING_JSON)\n",
    "    if sprint_id is None:\n",
    "        sprint_id = pick_latest_sprint_key(planning)\n",
    "\n",
    "    bucket = planning[sprint_id]\n",
    "    s_start = datetime.strptime(bucket[\"sprint_inicio_global\"], \"%Y-%m-%d\").replace(hour=0, minute=0, second=0)\n",
    "    s_end   = datetime.strptime(bucket[\"sprint_fin_global\"],    \"%Y-%m-%d\").replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    print(f\"‚û°Ô∏è  Sprint: {sprint_id} | Ventana: {s_start.date()} ‚Üí {s_end.date()} (incl.)\")\n",
    "\n",
    "    keys = extract_issue_keys(planning, sprint_id)\n",
    "    if not keys:\n",
    "        raise SystemExit(\"No hay issues en planificaci√≥n.\")\n",
    "    print(f\"üßæ Issues a consultar: {len(keys)}\")\n",
    "\n",
    "    jira_map = fetch_issues_status(keys)\n",
    "    valid_keys = [k for k in keys if isinstance(k, str) and \"-\" in k and not k.startswith(\"S\")]\n",
    "    worklogs = fetch_worklogs_window(valid_keys, s_start, s_end, jira_map=jira_map)\n",
    "\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    issues_out = {}\n",
    "    resumen_por_estado = defaultdict(int)\n",
    "    resumen_por_dev    = defaultdict(int)\n",
    "\n",
    "    # Aux: mapa dev por issue desde planning\n",
    "    dev_map_issue = {}\n",
    "    for dev, dev_data in bucket[\"detalle_por_desarrollador\"].items():\n",
    "        for w in (\"datos_semana_1\", \"datos_semana_2\"):\n",
    "            detail = dev_data[w].get(\"issues_detail\", []) or []\n",
    "            for it in detail:\n",
    "                if not it: continue\n",
    "                k = it.get(\"key\") \n",
    "                if not k: continue\n",
    "                dev_map_issue[str(k)] = dev\n",
    "\n",
    "            if not detail and dev_data[w].get(\"issues\"):\n",
    "                for k in dev_data[w][\"issues\"]:\n",
    "                    dev_map_issue[str(k)] = dev\n",
    "\n",
    "    for k in keys:\n",
    "        j = jira_map.get(k, {})\n",
    "        dev = dev_map_issue.get(k)\n",
    "        status = j.get(\"status\", \"DESCONOCIDO\")\n",
    "        wl = worklogs.get(k, {\"total_sec\": 0, \"by_author\": {}})\n",
    "\n",
    "        # NUEVO: horas reales por tarjeta desde el custom (entero = horas)\n",
    "        raw_cf = j.get(REAL_HOURS_CF)\n",
    "        hours_cf = 0.0\n",
    "        if isinstance(raw_cf, (int, float)):\n",
    "            hours_cf = float(raw_cf)\n",
    "        elif isinstance(raw_cf, str) and raw_cf.strip().replace('.', '', 1).isdigit():\n",
    "            hours_cf = float(raw_cf.strip())\n",
    "\n",
    "        hours_wl = (wl.get(\"total_sec\", 0) or 0) / 3600.0\n",
    "\n",
    "        if hours_cf > 0:\n",
    "            real_hours_issue = hours_cf\n",
    "            real_source = REAL_HOURS_CF\n",
    "        else:\n",
    "            real_hours_issue = hours_wl\n",
    "            real_source = \"worklogs_window\"\n",
    "\n",
    "        DONE_STATUSES = {\n",
    "            \"Done\",\n",
    "            \"FINISH\",\n",
    "            \"TO DEPLOY\",\n",
    "            \"Rechazado\",\n",
    "            \"PENDING REVIEW\",\n",
    "            \"REVIEW ON HOLD\",\n",
    "            \"REVIEW ON PAUSE\",\n",
    "            \"REVIEWING\",\n",
    "            \"TO DEPLOY QA\",\n",
    "        }\n",
    "\n",
    "        status_name = (j.get(\"status\") or \"\").strip()\n",
    "        status_cat = (j.get(\"status_category\") or \"\").strip()\n",
    "        issues_out[k] = {\n",
    "            \"developer\": dev,\n",
    "            \"summary\": j.get(\"summary\"),\n",
    "            \"status\": status,\n",
    "            \"status_category\": j.get(\"status_category\"),\n",
    "            \"assignee\": j.get(\"assignee\"),\n",
    "            \"priority\": j.get(\"priority\"),\n",
    "            \"duedate\": j.get(\"duedate\"),\n",
    "            \"updated\": j.get(\"updated\"),\n",
    "            \"resolutiondate\": j.get(\"resolutiondate\"),\n",
    "            \"worklogs\": wl,\n",
    "            \"customfield_10046_raw\": raw_cf,      # <-- NUEVO\n",
    "            \"real_hours_issue\": real_hours_issue, # <-- NUEVO\n",
    "            \"real_hours_source\": real_source,     # <-- NUEVO\n",
    "            \"done\": (status_cat == \"Done\") or (status_name in DONE_STATUSES),\n",
    "            \"done_before_due\": False,  # se setea abajo si aplica\n",
    "            \"capturado_en\": now,\n",
    "        }\n",
    "\n",
    "        # done_before_due\n",
    "        try:\n",
    "            if j.get(\"resolutiondate\") and j.get(\"duedate\"):\n",
    "                res = datetime.fromisoformat(j[\"resolutiondate\"].replace(\"Z\", \"+00:00\")).astimezone(TZ)\n",
    "                due = datetime.strptime(j[\"duedate\"], \"%Y-%m-%d\").replace(tzinfo=TZ, hour=23, minute=59, second=59)\n",
    "                issues_out[k][\"done_before_due\"] = res <= due\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        resumen_por_estado[status] += 1\n",
    "        if dev: resumen_por_dev[dev] += 1\n",
    "\n",
    "    payload = {\n",
    "        \"generado_en\": now,\n",
    "        \"sprint_id\": sprint_id,\n",
    "        \"sprint_inicio_global\": bucket[\"sprint_inicio_global\"],\n",
    "        \"sprint_fin_global\": bucket[\"sprint_fin_global\"],\n",
    "        \"issues\": issues_out,\n",
    "        \"resumen\": {\n",
    "            \"por_estado\": dict(resumen_por_estado),\n",
    "            \"por_desarrollador\": dict(resumen_por_dev),\n",
    "            \"total\": len(keys),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    store = {}\n",
    "    if os.path.exists(REPORT_JSON):\n",
    "        try:\n",
    "            with open(REPORT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "                store = json.load(f)\n",
    "        except Exception:\n",
    "            store = {}\n",
    "    store[sprint_id] = payload\n",
    "\n",
    "    tmp = REPORT_JSON + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(store, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, REPORT_JSON)\n",
    "\n",
    "    print(f\"[OK] Reporte escrito en {REPORT_JSON}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(SPRINT_A_CALCULAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63fb0beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] KPIs escritos en kpis_it.json\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Cruza bt_it_sprint_planning.json + report_it.json y genera KPIs:\n",
    "- % tarjetas completadas\n",
    "- % horas cumplidas\n",
    "- Desviaci√≥n promedio de estimaci√≥n\n",
    "- % tareas replanificadas\n",
    "- Carga planificada por dev\n",
    "- Predictibilidad (% terminadas antes del due date)\n",
    "Salida: kpis_it.json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "PLANNING=\"bt_it_sprint_planning.json\"\n",
    "REPORT=\"report_it.json\"\n",
    "OUT=\"kpis_it.json\"\n",
    "\n",
    "DEV_MAP = {\n",
    "    # Opcional: mapear nombres del Sheet -> displayName de Jira\n",
    "    # \"Alan Mori - Carestino\": \"Alan Mori - Carestino\",\n",
    "}\n",
    "\n",
    "def parse_sprint_date(sprint_id: str) -> datetime:\n",
    "    m = re.match(r\"^S(\\d{8})$\", sprint_id)\n",
    "    return datetime.strptime(m.group(1), \"%Y%m%d\")\n",
    "\n",
    "def pick_latest_sprint_key(store: Dict[str, Any]) -> str:\n",
    "    keys = [k for k in store.keys() if re.match(r\"^S\\d{8}$\", k)]\n",
    "    keys.sort(key=lambda k: parse_sprint_date(k))\n",
    "    return keys[-1]\n",
    "\n",
    "def loadj(p):\n",
    "    if not os.path.exists(p): raise SystemExit(f\"No existe {p}\")\n",
    "    with open(p,\"r\",encoding=\"utf-8\") as f: return json.load(f)\n",
    "\n",
    "def main(sprint_id=None):\n",
    "    plan = loadj(PLANNING)\n",
    "    if sprint_id is None:\n",
    "        sprint_id = pick_latest_sprint_key(plan)\n",
    "    rep_store = loadj(REPORT)\n",
    "    if sprint_id not in rep_store:\n",
    "        raise SystemExit(f\"No hay reporte para {sprint_id} en {REPORT}\")\n",
    "\n",
    "    bucket = plan[sprint_id]\n",
    "    report = rep_store[sprint_id][\"issues\"]\n",
    "\n",
    "    # --- Acumuladores por dev ---\n",
    "    by_dev = defaultdict(lambda: {\n",
    "        \"planned_cards\":0,\n",
    "        \"planned_hours\":0.0,\n",
    "        \"done_cards\":0,\n",
    "        \"real_hours\":0.0,\n",
    "    })\n",
    "\n",
    "    # map ‚Äúplanned_hours por issue‚Äù\n",
    "    per_issue_planned = {}  # key -> planned_hours\n",
    "    # y dev por issue seg√∫n planificaci√≥n\n",
    "    dev_by_issue = {}\n",
    "\n",
    "    detail = bucket[\"detalle_por_desarrollador\"]\n",
    "    for dev, d in detail.items():\n",
    "        for w in (\"datos_semana_1\",\"datos_semana_2\"):\n",
    "            dd = d.get(w, {})\n",
    "            by_dev[dev][\"planned_cards\"] += len(dd.get(\"issues\", []))\n",
    "            by_dev[dev][\"planned_hours\"] += float(dd.get(\"horas_planificadas\", 0))\n",
    "            for it in dd.get(\"issues_detail\", []) or []:\n",
    "                k = (it or {}).get(\"key\")\n",
    "                if not k: continue\n",
    "                per_issue_planned[k] = float((it.get(\"planned_hours\") or 0))\n",
    "                dev_by_issue[k] = dev\n",
    "            # compat: si no hay issues_detail, usar lista simple\n",
    "            if not dd.get(\"issues_detail\") and dd.get(\"issues\"):\n",
    "                for k in dd.get(\"issues\"):\n",
    "                    dev_by_issue[k] = dev\n",
    "\n",
    "    # --- Sumar realizado por dev ---\n",
    "    for k, info in report.items():\n",
    "        dev = dev_by_issue.get(k) or info.get(\"developer\")\n",
    "        if not dev:\n",
    "            continue\n",
    "        if info.get(\"done\"):\n",
    "            by_dev[dev][\"done_cards\"] += 1\n",
    "\n",
    "        # Preferir horas reales del custom; fallback: worklogs\n",
    "        wl = info.get(\"worklogs\", {})\n",
    "        if isinstance(wl.get(\"by_author\"), dict) and any(wl[\"by_author\"].values()):\n",
    "            author_name = DEV_MAP.get(dev, dev)\n",
    "            real_hours = int(wl[\"by_author\"].get(author_name, 0)) / 3600.0\n",
    "        else:\n",
    "            # fallback si no hay worklogs (tarjeta sin registrar tiempo)\n",
    "            if isinstance(info.get(\"real_hours_issue\"), (int, float)) and info[\"real_hours_issue\"] > 0:\n",
    "                real_hours = float(info[\"real_hours_issue\"])\n",
    "            else:\n",
    "                real_hours = 0.0\n",
    "\n",
    "        by_dev[dev][\"real_hours\"] += real_hours\n",
    "\n",
    "    # --- KPIs por dev + desviaci√≥n promedio ---\n",
    "    kpis_dev = {}\n",
    "    for dev, v in by_dev.items():\n",
    "        pc, ph, dc, rh = v[\"planned_cards\"], v[\"planned_hours\"], v[\"done_cards\"], v[\"real_hours\"]\n",
    "        pct_cards = (dc/pc*100) if pc else None\n",
    "        pct_hours = (rh/ph*100) if ph else None\n",
    "\n",
    "        # desviaci√≥n promedio por issue planificada del dev\n",
    "        dev_issues = [k for k, d in dev_by_issue.items() if d == dev]\n",
    "        dev_devs = []\n",
    "        for k in dev_issues:\n",
    "            p = per_issue_planned.get(k, 0.0)\n",
    "            if p <= 0:\n",
    "                continue\n",
    "\n",
    "            # ‚úÖ Nuevo: preferir horas reales del customfield_10046 si existe\n",
    "            info_k = report.get(k, {})\n",
    "            if isinstance(info_k.get(\"real_hours_issue\"), (int, float)) and info_k[\"real_hours_issue\"] > 0:\n",
    "                real_issue = float(info_k[\"real_hours_issue\"])\n",
    "            else:\n",
    "                wl = info_k.get(\"worklogs\", {})\n",
    "                if isinstance(wl.get(\"by_author\"), dict):\n",
    "                    author_name = DEV_MAP.get(dev, dev)\n",
    "                    real_issue_sec = int(wl[\"by_author\"].get(author_name, 0))\n",
    "                else:\n",
    "                    real_issue_sec = int(wl.get(\"total_sec\", 0))\n",
    "                real_issue = real_issue_sec / 3600.0\n",
    "\n",
    "            dev_devs.append(abs(p - real_issue) / p)\n",
    "\n",
    "        avg_dev = (sum(dev_devs)/len(dev_devs)) if dev_devs else None\n",
    "\n",
    "        kpis_dev[dev] = {\n",
    "            \"tarjetas_planificadas\": pc,\n",
    "            \"tarjetas_realizadas\": dc,\n",
    "            \"porc_tarjetas_completadas\": round(pct_cards,1) if pct_cards is not None else None,\n",
    "            \"horas_planificadas\": round(ph,1),\n",
    "            \"horas_realizadas\": round(rh,1),\n",
    "            \"porc_horas_cumplidas\": round(pct_hours,1) if pct_hours is not None else None,\n",
    "            \"desviacion_promedio_estimacion\": round(avg_dev,3) if avg_dev is not None else None,\n",
    "        }\n",
    "\n",
    "    # --- Globales ---\n",
    "    all_keys = list(report.keys())\n",
    "    done_keys = [k for k, i in report.items() if i.get(\"done\")]\n",
    "    replan_pct = round((1 - len(done_keys)/len(all_keys))*100,1) if all_keys else None\n",
    "\n",
    "    completed_with_due = [k for k in done_keys if report[k].get(\"duedate\")]\n",
    "    ontime = [k for k in completed_with_due if report[k].get(\"done_before_due\")]\n",
    "    predict = round(len(ontime)/len(completed_with_due)*100,1) if completed_with_due else None\n",
    "\n",
    "    out = {\n",
    "        \"sprint_id\": sprint_id,\n",
    "        \"kpis_por_desarrollador\": kpis_dev,\n",
    "        \"kpis_globales\": {\n",
    "            \"porc_tareas_replanificadas\": replan_pct,\n",
    "            \"predictibilidad_por_due_date\": predict,\n",
    "            \"carga_planificada_por_dev\": {d: kpis_dev[d][\"horas_planificadas\"] for d in kpis_dev},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    store = {}\n",
    "    if os.path.exists(OUT):\n",
    "        try:\n",
    "            with open(OUT, \"r\", encoding=\"utf-8\") as f:\n",
    "                store = json.load(f)\n",
    "        except Exception:\n",
    "            store = {}\n",
    "    store[sprint_id] = out\n",
    "\n",
    "    tmp = OUT + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(store, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, OUT)\n",
    "\n",
    "    print(f\"[OK] KPIs escritos en {OUT}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main(SPRINT_A_CALCULAR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
