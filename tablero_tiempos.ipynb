{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd95a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import JQL, FIELDS, BASE_URL, JIRA_DOMAIN,EMAIL, MAX_RESULTS, MODULE_DEVS, VALID_STATUSES_BT, MAIL_BT_MAP, DAILY_BT_HOURS,MIN_BT_PROJECT_RATIO,PROJECT_MAP, DEFAULT_END_DATE, DEFAULT_END_DATE_with_timezone, DEFAULT_START_DATE,DEFAULT_START_DATE_with_timezone\n",
    "from token_hidden import API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7210f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 (Seed) Keys obtenidas: 2016 (rango IT-19773..IT-23773)\n",
      "…procesados 25/2016\n",
      "…procesados 50/2016\n",
      "…procesados 75/2016\n",
      "…procesados 100/2016\n",
      "…procesados 125/2016\n",
      "…procesados 150/2016\n",
      "…procesados 175/2016\n",
      "…procesados 200/2016\n",
      "…procesados 225/2016\n",
      "…procesados 250/2016\n",
      "…procesados 275/2016\n",
      "…procesados 300/2016\n",
      "…procesados 325/2016\n",
      "…procesados 350/2016\n",
      "…procesados 375/2016\n",
      "…procesados 400/2016\n",
      "…procesados 425/2016\n",
      "…procesados 450/2016\n",
      "…procesados 475/2016\n",
      "…procesados 500/2016\n",
      "…procesados 525/2016\n",
      "…procesados 550/2016\n",
      "…procesados 575/2016\n",
      "…procesados 600/2016\n",
      "…procesados 625/2016\n",
      "…procesados 650/2016\n",
      "…procesados 675/2016\n",
      "…procesados 700/2016\n",
      "…procesados 725/2016\n",
      "…procesados 750/2016\n",
      "…procesados 775/2016\n",
      "…procesados 800/2016\n",
      "…procesados 825/2016\n",
      "…procesados 850/2016\n",
      "…procesados 875/2016\n",
      "…procesados 900/2016\n",
      "…procesados 925/2016\n",
      "…procesados 950/2016\n",
      "…procesados 975/2016\n",
      "…procesados 1000/2016\n",
      "…procesados 1025/2016\n",
      "…procesados 1050/2016\n",
      "…procesados 1075/2016\n",
      "…procesados 1100/2016\n",
      "…procesados 1125/2016\n",
      "…procesados 1150/2016\n",
      "…procesados 1175/2016\n",
      "…procesados 1200/2016\n",
      "…procesados 1225/2016\n",
      "…procesados 1250/2016\n",
      "…procesados 1275/2016\n",
      "…procesados 1300/2016\n",
      "…procesados 1325/2016\n",
      "…procesados 1350/2016\n",
      "…procesados 1375/2016\n",
      "…procesados 1400/2016\n",
      "…procesados 1425/2016\n",
      "…procesados 1450/2016\n",
      "…procesados 1475/2016\n",
      "…procesados 1500/2016\n",
      "…procesados 1525/2016\n",
      "…procesados 1550/2016\n",
      "…procesados 1575/2016\n",
      "…procesados 1600/2016\n",
      "…procesados 1625/2016\n",
      "…procesados 1650/2016\n",
      "…procesados 1675/2016\n",
      "…procesados 1700/2016\n",
      "…procesados 1725/2016\n",
      "…procesados 1750/2016\n",
      "…procesados 1775/2016\n",
      "…procesados 1800/2016\n",
      "…procesados 1825/2016\n",
      "…procesados 1850/2016\n",
      "…procesados 1875/2016\n",
      "…procesados 1900/2016\n",
      "…procesados 1925/2016\n",
      "…procesados 1950/2016\n",
      "…procesados 1975/2016\n",
      "…procesados 2000/2016\n",
      "✅ CSV generado: cycle_times.csv (294 filas)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "SITE = \"https://team-1583163151751.atlassian.net\"\n",
    "EMAIL = os.getenv(\"JIRA_EMAIL\") or EMAIL     # deja tu variable existente\n",
    "API_TOKEN = os.getenv(\"JIRA_API_TOKEN\") or API_TOKEN\n",
    "\n",
    "AUTH = HTTPBasicAuth(EMAIL, API_TOKEN)\n",
    "HEADERS = {\"Accept\": \"application/json\"}\n",
    "TIMEOUT = 30\n",
    "MAX_WORKERS = int(os.getenv(\"MAX_WORKERS\", \"8\"))  # concurrencia para fetch de issues\n",
    "\n",
    "# Endpoints\n",
    "ISSUE_GET = SITE + \"/rest/api/3/issue/{key}?expand=changelog\"\n",
    "AGILE_BOARDS = SITE + \"/rest/agile/1.0/board\"\n",
    "AGILE_BOARD_ISSUES = SITE + \"/rest/agile/1.0/board/{boardId}/issue\"\n",
    "\n",
    "# ===================== HTTP helpers =====================\n",
    "SESSION = requests.Session()\n",
    "\n",
    "def http_get(url: str, params: Dict = None, ok_404: bool = False):\n",
    "    \"\"\"GET con reintentos y backoff exponencial; maneja 429/5xx.\"\"\"\n",
    "    backoff = 0.5\n",
    "    for attempt in range(6):\n",
    "        r = SESSION.get(url, headers=HEADERS, auth=AUTH, params=params, timeout=TIMEOUT)\n",
    "        # 2xx\n",
    "        if 200 <= r.status_code < 300:\n",
    "            return r\n",
    "        # 404 permitido\n",
    "        if ok_404 and r.status_code == 404:\n",
    "            return r\n",
    "        # 429 o 5xx -> retry con backoff + Retry-After si viene\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            wait = r.headers.get(\"Retry-After\")\n",
    "            wait_s = float(wait) if wait and wait.isdigit() else backoff\n",
    "            time.sleep(wait_s)\n",
    "            backoff = min(backoff * 2, 8.0)\n",
    "            continue\n",
    "        # otros códigos -> error inmediato\n",
    "        r.raise_for_status()\n",
    "    r.raise_for_status()\n",
    "    return r  # never reached\n",
    "\n",
    "# ===================== UTIL =====================\n",
    "def parse_jira_dt(s: str) -> datetime:\n",
    "    # Ej: 2025-09-30T09:50:38.240-0300\n",
    "    return datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "def extract_time_between_statuses(issue_json: dict, from_status: str, to_status: str):\n",
    "    ch = issue_json.get(\"changelog\", {}) or {}\n",
    "    histories = ch.get(\"histories\", []) or []\n",
    "    from_date, to_date = None, None\n",
    "    for h in histories:\n",
    "        created = h.get(\"created\")\n",
    "        for item in h.get(\"items\", []) or []:\n",
    "            if item.get(\"field\") == \"status\":\n",
    "                if item.get(\"toString\") == from_status and not from_date:\n",
    "                    from_date = created\n",
    "                if item.get(\"toString\") == to_status and not to_date:\n",
    "                    to_date = created\n",
    "    if from_date and to_date:\n",
    "        dt_from = parse_jira_dt(from_date)\n",
    "        dt_to = parse_jira_dt(to_date)\n",
    "        hours = (dt_to - dt_from).total_seconds() / 3600\n",
    "        return {\"from\": from_date, \"to\": to_date, \"hours\": round(hours, 2)}\n",
    "    return None\n",
    "\n",
    "def fetch_issue_with_changelog(key: str) -> Optional[dict]:\n",
    "    url = ISSUE_GET.format(key=key)\n",
    "    r = http_get(url, ok_404=True)\n",
    "    if r.status_code == 404:\n",
    "        return None\n",
    "    return r.json()\n",
    "\n",
    "# ===================== RUTA A: AGILE API =====================\n",
    "def get_first_board_for_project(project_key: str) -> Optional[int]:\n",
    "    # probamos kanban\n",
    "    r = http_get(AGILE_BOARDS, params={\"projectKeyOrId\": project_key, \"type\": \"kanban\"})\n",
    "    data = r.json()\n",
    "    if data.get(\"values\"):\n",
    "        return data[\"values\"][0][\"id\"]\n",
    "    # probamos scrum\n",
    "    r = http_get(AGILE_BOARDS, params={\"projectKeyOrId\": project_key, \"type\": \"scrum\"})\n",
    "    data = r.json()\n",
    "    if data.get(\"values\"):\n",
    "        return data[\"values\"][0][\"id\"]\n",
    "    return None\n",
    "\n",
    "def list_issue_keys_via_board(board_id: int, jql: str | None = None, page_size: int = 50) -> List[str]:\n",
    "    start_at = 0\n",
    "    keys = []\n",
    "    while True:\n",
    "        params = {\"startAt\": start_at, \"maxResults\": page_size, \"fields\": \"key\"}\n",
    "        if jql:\n",
    "            params[\"jql\"] = jql  # p.ej. created >= -90d\n",
    "        r = http_get(AGILE_BOARD_ISSUES.format(boardId=board_id), params=params)\n",
    "        data = r.json()\n",
    "        batch = [i[\"key\"] for i in (data.get(\"issues\") or [])]\n",
    "        if not batch:\n",
    "            break\n",
    "        keys.extend(batch)\n",
    "        start_at += len(batch)\n",
    "        total = data.get(\"total\", start_at)\n",
    "        if start_at >= total:\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "    return keys\n",
    "\n",
    "# ===================== RUTA B: RANGO DESDE SEMILLA =====================\n",
    "def probe_key_exists(project_key: str, number: int) -> bool:\n",
    "    key = f\"{project_key}-{number}\"\n",
    "    url = ISSUE_GET.format(key=key)\n",
    "    r = http_get(url, ok_404=True)\n",
    "    if r.status_code == 200:\n",
    "        return True\n",
    "    if r.status_code == 404:\n",
    "        return False\n",
    "    r.raise_for_status()\n",
    "    return False\n",
    "\n",
    "def discover_keys_around_seed(project_key: str, seed_number: int, span_each_side: int = 500, concurrency: int = MAX_WORKERS) -> List[str]:\n",
    "    \"\"\"\n",
    "    Escanea IT-(seed-span .. seed+span) en paralelo, guardando solo las existentes (200).\n",
    "    \"\"\"\n",
    "    numbers = list(range(max(1, seed_number - span_each_side), seed_number + span_each_side + 1))\n",
    "    keys_found = []\n",
    "    with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "        fut_map = {ex.submit(probe_key_exists, project_key, n): n for n in numbers}\n",
    "        for fut in as_completed(fut_map):\n",
    "            n = fut_map[fut]\n",
    "            try:\n",
    "                if fut.result():\n",
    "                    keys_found.append(f\"{project_key}-{n}\")\n",
    "            except requests.HTTPError:\n",
    "                time.sleep(0.3)  # bajamos ritmo si hubo error\n",
    "    keys_found.sort(key=lambda k: int(k.split(\"-\")[1]))\n",
    "    return keys_found\n",
    "\n",
    "# ===================== EXPORT =====================\n",
    "def row_from_issue(issue_json: dict, cycle: dict) -> dict:\n",
    "    fields = issue_json.get(\"fields\", {}) or {}\n",
    "    assignee = fields.get(\"assignee\") or {}\n",
    "    issue_type = fields.get(\"issuetype\") or {}\n",
    "    return {\n",
    "        \"issueKey\": issue_json.get(\"key\"),\n",
    "        \"issueType\": issue_type.get(\"name\"),\n",
    "        \"assignee\": assignee.get(\"displayName\"),\n",
    "        \"summary\": fields.get(\"summary\"),\n",
    "        \"from_date\": cycle[\"from\"],\n",
    "        \"to_date\": cycle[\"to\"],\n",
    "        \"hours\": cycle[\"hours\"],\n",
    "    }\n",
    "\n",
    "def export_cycle_times(keys: List[str], from_status: str, to_status: str, csv_out: str, concurrency: int = MAX_WORKERS):\n",
    "    rows = []\n",
    "\n",
    "    def process_key(k: str) -> Optional[dict]:\n",
    "        issue = fetch_issue_with_changelog(k)\n",
    "        if not issue:\n",
    "            return None\n",
    "        cyc = extract_time_between_statuses(issue, from_status, to_status)\n",
    "        if not cyc:\n",
    "            return None\n",
    "        return row_from_issue(issue, cyc)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=concurrency) as ex:\n",
    "        futs = [ex.submit(process_key, key) for key in keys]\n",
    "        done = 0\n",
    "        for fut in as_completed(futs):\n",
    "            row = fut.result()\n",
    "            if row:\n",
    "                rows.append(row)\n",
    "            done += 1\n",
    "            if done % 25 == 0:\n",
    "                print(f\"…procesados {done}/{len(keys)}\")\n",
    "\n",
    "    rows.sort(key=lambda r: (r[\"to_date\"] or \"\", r[\"issueKey\"] or \"\"))\n",
    "    with open(csv_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"issueKey\", \"issueType\", \"assignee\", \"summary\", \"from_date\", \"to_date\", \"hours\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "    print(f\"✅ CSV generado: {csv_out} ({len(rows)} filas)\")\n",
    "\n",
    "# ===================== MAIN =====================\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT = \"IT\"\n",
    "    FROM_STATUS = \"Pendiente de estimación\"\n",
    "    TO_STATUS = \"Estimada\"\n",
    "    CSV_OUT = \"cycle_times.csv\"\n",
    "\n",
    "    # ---- Opción A: Agile API (recomendada si hay board) ----\n",
    "    keys: List[str] = []\n",
    "    board_id = get_first_board_for_project(PROJECT)\n",
    "    if board_id:\n",
    "        # TIP: podés acotar con jql=\"created >= -90d\" para menos volumen\n",
    "        keys = list_issue_keys_via_board(board_id, jql=None, page_size=50)\n",
    "        print(f\"🔑 (Agile) Keys obtenidas: {len(keys)}\")\n",
    "\n",
    "    # ---- Opción B: Semilla (si no hay board/permiso) ----\n",
    "    if not keys:\n",
    "        SEED = 21773     # confirmaste que IT-21773 existe\n",
    "        SPAN = 2000      # +/- 1000 alrededor de la semilla\n",
    "        keys = discover_keys_around_seed(PROJECT, SEED, span_each_side=SPAN, concurrency=MAX_WORKERS)\n",
    "        print(f\"🔑 (Seed) Keys obtenidas: {len(keys)} (rango IT-{SEED-SPAN}..IT-{SEED+SPAN})\")\n",
    "\n",
    "    if not keys:\n",
    "        print(\"⚠️ No se encontraron issues. Revisá permisos, project y el board/rango.\")\n",
    "    else:\n",
    "        export_cycle_times(keys, FROM_STATUS, TO_STATUS, csv_out=CSV_OUT, concurrency=MAX_WORKERS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
