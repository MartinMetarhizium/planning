{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27158e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import JQL, FIELDS, BASE_URL, JIRA_DOMAIN,EMAIL, MAX_RESULTS, MODULE_DEVS, VALID_STATUSES, MAIL_MAP, DAILY_HOURS,MIN_PROJECT_RATIO,PROJECT_MAP, DEFAULT_END_DATE, DEFAULT_END_DATE_with_timezone, DEFAULT_START_DATE,DEFAULT_START_DATE_with_timezone\n",
    "from token_hidden import API_TOKEN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "942e7ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Linked keys found: 5776\n",
      "ðŸ’¾ JSON enriched saved: jira_it_issues.json\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://team-1583163151751.atlassian.net/rest/api/3/search/jql",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 308\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_epics\n\u001b[1;32m--> 308\u001b[0m epics \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_epics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# Guardar en archivo si lo deseas\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepics_due_lookup.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[11], line 283\u001b[0m, in \u001b[0;36mfetch_epics\u001b[1;34m()\u001b[0m\n\u001b[0;32m    280\u001b[0m     story_payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnextPageToken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m next_story\n\u001b[0;32m    282\u001b[0m sr \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(SEARCH_JQL_URL, headers\u001b[38;5;241m=\u001b[39mHEADERS, auth\u001b[38;5;241m=\u001b[39mAUTH, json\u001b[38;5;241m=\u001b[39mstory_payload, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m \u001b[43msr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m sd \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m st \u001b[38;5;129;01min\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missues\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "File \u001b[1;32mc:\\Users\\Martin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://team-1583163151751.atlassian.net/rest/api/3/search/jql"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from typing import List, Dict, Any, Set\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# ========= Config =========\n",
    "\n",
    "\n",
    "AUTH = HTTPBasicAuth(EMAIL, API_TOKEN)\n",
    "HEADERS = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "\n",
    "# JQL principal (ajustar a lo que uses)\n",
    "JQL = \"\"\"\n",
    "project = IT\n",
    "AND issuetype NOT IN (Epic, Sub-task, Subtarea)\n",
    "ORDER BY priority DESC, duedate ASC\n",
    "\"\"\".strip()\n",
    "\n",
    "# Campos que querÃ©s en los issues \"base\"\n",
    "MAIN_FIELDS = [\n",
    "    \"summary\",\n",
    "    \"project\",\n",
    "    \"reporter\",\n",
    "    \"assignee\",\n",
    "    \"status\",\n",
    "    \"priority\",\n",
    "    \"issuetype\",\n",
    "    \"timetracking\",\n",
    "    \"duedate\",\n",
    "    \"customfield_10016\",  # Story Points\n",
    "    \"customfield_10212\",  # MÃ³dulo (ejemplo)\n",
    "    \"customfield_10214\",\n",
    "    \"customfield_10442\",\n",
    "    \"customfield_10608\",\n",
    "    \"issuelinks\"          # Â¡necesario para ver vÃ­nculos!\n",
    "]\n",
    "\n",
    "# Campos que querÃ©s traer del issue vinculado (ademÃ¡s de summary/status/priority, etc.)\n",
    "WANTED_FIELDS = [\n",
    "    \"customfield_10209\",\n",
    "]\n",
    "\n",
    "MAX_RESULTS = 100\n",
    "CHUNK_LINKED = 50  # tamaÃ±o de lote para buscar issues vinculados\n",
    "OUT_JSON = \"jira_it_issues.json\"\n",
    "  # opcional\n",
    "JIRA_API_ROOT = \"https://team-1583163151751.atlassian.net/rest/api/3\"\n",
    "SEARCH_JQL_URL = f\"{JIRA_API_ROOT}/search/jql\"  \n",
    "\n",
    "# ========= Funciones utilitarias =========\n",
    "def fetch_issues_paged(jql: str, fields: list[str]) -> list[dict]:\n",
    "    all_issues = []\n",
    "    next_token = None\n",
    "    while True:\n",
    "        payload = {\n",
    "            \"jql\": jql,\n",
    "            \"fields\": fields,          # lista (incluÃ­ siempre 'issuelinks')\n",
    "            \"maxResults\": MAX_RESULTS, # mismo nombre\n",
    "        }\n",
    "        if next_token:\n",
    "            payload[\"nextPageToken\"] = next_token\n",
    "\n",
    "        resp = requests.post(SEARCH_JQL_URL, headers=HEADERS, auth=AUTH, json=payload, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        issues = data.get(\"issues\", [])\n",
    "        all_issues.extend(issues)\n",
    "\n",
    "        # Nueva paginaciÃ³n\n",
    "        if data.get(\"isLast\", False):\n",
    "            break\n",
    "        next_token = data.get(\"nextPageToken\")\n",
    "        if not next_token:  # fallback defensivo\n",
    "            break\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    return all_issues\n",
    "\n",
    "\n",
    "def collect_linked_issue_keys(issues: List[Dict[str, Any]], only_types: List[str] = None) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Recolecta todas las keys de inward/outward de issuelinks.\n",
    "    only_types: si lo pasÃ¡s, filtrÃ¡s por nombre de tipo de vÃ­nculo (p.ej. [\"Problem/Incident\"])\n",
    "    \"\"\"\n",
    "    keys: Set[str] = set()\n",
    "    for it in issues:\n",
    "        links = it.get(\"fields\", {}).get(\"issuelinks\", []) or []\n",
    "        for link in links:\n",
    "            if only_types and link.get(\"type\", {}).get(\"name\") not in only_types:\n",
    "                continue\n",
    "            for side in (\"outwardIssue\", \"inwardIssue\"):\n",
    "                if side in link and \"key\" in link[side]:\n",
    "                    keys.add(link[side][\"key\"])\n",
    "    print(f\"ðŸ”— Linked keys found: {len(keys)}\")\n",
    "    return keys\n",
    "\n",
    "\n",
    "def fetch_issues_by_keys(keys: list[str], fields: list[str], chunk: int = CHUNK_LINKED) -> dict[str, dict]:\n",
    "    out = {}\n",
    "    for i in range(0, len(keys), chunk):\n",
    "        slice_keys = keys[i:i+chunk]\n",
    "        jql = f\"issuekey in ({','.join(slice_keys)})\"\n",
    "\n",
    "        next_token = None\n",
    "        while True:\n",
    "            payload = {\n",
    "                \"jql\": jql,\n",
    "                \"fields\": fields,\n",
    "                \"maxResults\": 100,\n",
    "            }\n",
    "            if next_token:\n",
    "                payload[\"nextPageToken\"] = next_token\n",
    "\n",
    "            resp = requests.post(SEARCH_JQL_URL, headers=HEADERS, auth=AUTH, json=payload, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "\n",
    "            for issue in data.get(\"issues\", []):\n",
    "                out[issue[\"key\"]] = issue\n",
    "\n",
    "            if data.get(\"isLast\", False):\n",
    "                break\n",
    "            next_token = data.get(\"nextPageToken\")\n",
    "            if not next_token:\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "    return out\n",
    "\n",
    "def enrich_issue_links_with_fields(issues: List[Dict[str, Any]],\n",
    "                                   linked_map: Dict[str, Dict[str, Any]],\n",
    "                                   fields_to_copy: List[str]) -> None:\n",
    "    \"\"\"Inyecta 'fields_to_copy' dentro de outward/inwardIssue.fields si NO existen aÃºn en el issue base.\"\"\"\n",
    "    for it in issues:\n",
    "        links = it.get(\"fields\", {}).get(\"issuelinks\", []) or []\n",
    "        for link in links:\n",
    "            for side in (\"outwardIssue\", \"inwardIssue\"):\n",
    "                if side in link and \"key\" in link[side]:\n",
    "                    k = link[side][\"key\"]\n",
    "                    if k in linked_map:\n",
    "                        link[side].setdefault(\"fields\", {})\n",
    "                        src_fields = linked_map[k].get(\"fields\", {})\n",
    "                        for f in fields_to_copy:\n",
    "                            # â›”ï¸ Solo insertar si NO estaba definido antes\n",
    "                            if f not in link[side][\"fields\"]:\n",
    "                                link[side][\"fields\"][f] = src_fields.get(f)\n",
    "\n",
    "def check_links_integrity(before: List[Dict[str, Any]], after: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"Verifica que los vÃ­nculos crÃ­ticos como 'Blocks' no se hayan perdido tras enriquecer.\"\"\"\n",
    "    for b_issue, a_issue in zip(before, after):\n",
    "        b_links = b_issue.get(\"fields\", {}).get(\"issuelinks\", [])\n",
    "        a_links = a_issue.get(\"fields\", {}).get(\"issuelinks\", [])\n",
    "        b_blocks = {(l.get(\"type\", {}).get(\"name\"), l.get(\"outwardIssue\", {}).get(\"key\"), l.get(\"inwardIssue\", {}).get(\"key\")) for l in b_links if l.get(\"type\", {}).get(\"name\") == \"Blocks\"}\n",
    "        a_blocks = {(l.get(\"type\", {}).get(\"name\"), l.get(\"outwardIssue\", {}).get(\"key\"), l.get(\"inwardIssue\", {}).get(\"key\")) for l in a_links if l.get(\"type\", {}).get(\"name\") == \"Blocks\"}\n",
    "\n",
    "        if b_blocks != a_blocks:\n",
    "            print(f\"ðŸš¨ WARNING: Issue {b_issue['key']} had Blocks links altered!\")\n",
    "\n",
    "def flatten_links_to_csv(issues: List[Dict[str, Any]],\n",
    "                         csv_path: str,\n",
    "                         sides: List[str] = (\"outwardIssue\", \"inwardIssue\"),\n",
    "                         fields_for_flat: List[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Aplana vÃ­nculos a CSV: una fila por (issue base, vÃ­nculo).\n",
    "    fields_for_flat: columnas a sacar del vinculado (ademÃ¡s de key y type).\n",
    "    \"\"\"\n",
    "    if fields_for_flat is None:\n",
    "        fields_for_flat = [\"summary\", \"status\", \"priority\", \"duedate\", \"customfield_10016\"]\n",
    "\n",
    "    rows = []\n",
    "    for it in issues:\n",
    "        base_key = it.get(\"key\")\n",
    "        base_summary = it.get(\"fields\", {}).get(\"summary\")\n",
    "        links = it.get(\"fields\", {}).get(\"issuelinks\", []) or []\n",
    "        for link in links:\n",
    "            link_type = link.get(\"type\", {}).get(\"name\")\n",
    "            for side in sides:\n",
    "                linked = link.get(side)\n",
    "                if not linked:\n",
    "                    continue\n",
    "                lkey = linked.get(\"key\")\n",
    "                lf = (linked.get(\"fields\") or {})\n",
    "                row = {\n",
    "                    \"base_issue\": base_key,\n",
    "                    \"base_summary\": base_summary,\n",
    "                    \"link_type\": link_type or \"\",\n",
    "                    \"link_side\": side,\n",
    "                    \"linked_issue\": lkey,\n",
    "                }\n",
    "                # Agregar columnas pedidas\n",
    "                # status/priority vienen como objetos; sacar el \"name\" si existe\n",
    "                for col in fields_for_flat:\n",
    "                    val = lf.get(col)\n",
    "                    if isinstance(val, dict) and \"name\" in val:\n",
    "                        val = val[\"name\"]\n",
    "                    row[col] = val\n",
    "                rows.append(row)\n",
    "\n",
    "    # Escribir CSV\n",
    "    fieldnames = [\"base_issue\", \"base_summary\", \"link_type\", \"link_side\", \"linked_issue\"] + fields_for_flat\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"ðŸ“„ Links CSV saved: {csv_path} ({len(rows)} rows)\")\n",
    "\n",
    "\n",
    "# ========= Main =========\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Descargar issues base (con issuelinks)\n",
    "    base_issues = fetch_issues_paged(JQL, MAIN_FIELDS)\n",
    "\n",
    "    # 2) Recolectar keys de issues vinculados\n",
    "    linked_keys = collect_linked_issue_keys(base_issues, only_types=None)  # podÃ©s filtrar por tipo si querÃ©s\n",
    "\n",
    "    # 3) Traer info de issues vinculados (campos personalizados deseados)\n",
    "    linked_map = {}\n",
    "    if linked_keys:\n",
    "        linked_map = fetch_issues_by_keys(\n",
    "            list(linked_keys),\n",
    "            fields=WANTED_FIELDS,\n",
    "        )\n",
    "\n",
    "    # 4) Enriquecer los vÃ­nculos en la estructura original\n",
    "    base_issues_before = base_issues\n",
    "    if linked_map:\n",
    "        enrich_issue_links_with_fields(base_issues, linked_map, WANTED_FIELDS)\n",
    "\n",
    "    check_links_integrity(base_issues_before, base_issues)\n",
    "    # 5) Guardar JSON enriquecido\n",
    "    with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"issues\": base_issues}, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ðŸ’¾ JSON enriched saved: {OUT_JSON}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Download epics\n",
    "def fetch_epics():\n",
    "    all_epics = {}\n",
    "\n",
    "    # Epics del proyecto\n",
    "    jql_epics = 'project = IT AND issuetype = Epic'\n",
    "    for_epics = {\"jql\": jql_epics, \"fields\": [\"key\",\"duedate\",\"summary\"], \"maxResults\": 100}\n",
    "\n",
    "    next_token = None\n",
    "    while True:\n",
    "        payload = dict(for_epics, **({\"nextPageToken\": next_token} if next_token else {}))\n",
    "        r = requests.post(SEARCH_JQL_URL, headers=HEADERS, auth=AUTH, json=payload, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        for issue in data.get(\"issues\", []):\n",
    "            epic_key = issue[\"key\"]\n",
    "            f = issue.get(\"fields\", {}) or {}\n",
    "            all_epics[epic_key] = {\n",
    "                \"due_date\": f.get(\"duedate\"),\n",
    "                \"summary\": f.get(\"summary\", \"\"),\n",
    "                \"tasks\": []\n",
    "            }\n",
    "\n",
    "            # Historias de esa Ã©pica\n",
    "            jql_stories = f'\"Epic Link\" = {epic_key}'\n",
    "            next_story = None\n",
    "            while True:\n",
    "                story_payload = {\n",
    "                    \"jql\": jql_stories,\n",
    "                    \"fields\": [\"key\",\"customfield_10016\",\"status\"],\n",
    "                    \"maxResults\": 100\n",
    "                }\n",
    "                if next_story:\n",
    "                    story_payload[\"nextPageToken\"] = next_story\n",
    "\n",
    "                sr = requests.post(SEARCH_JQL_URL, headers=HEADERS, auth=AUTH, json=story_payload, timeout=30)\n",
    "                sr.raise_for_status()\n",
    "                sd = sr.json()\n",
    "\n",
    "                for st in sd.get(\"issues\", []):\n",
    "                    sf = st.get(\"fields\", {}) or {}\n",
    "                    all_epics[epic_key][\"tasks\"].append({\n",
    "                        \"key\": st[\"key\"],\n",
    "                        \"story_points\": sf.get(\"customfield_10016\"),\n",
    "                        \"status\": (sf.get(\"status\") or {}).get(\"name\", \"Sin estado\")\n",
    "                    })\n",
    "\n",
    "                if sd.get(\"isLast\", False):\n",
    "                    break\n",
    "                next_story = sd.get(\"nextPageToken\")\n",
    "                if not next_story:\n",
    "                    break\n",
    "\n",
    "        if data.get(\"isLast\", False):\n",
    "            break\n",
    "        next_token = data.get(\"nextPageToken\")\n",
    "        if not next_token:\n",
    "            break\n",
    "\n",
    "    return all_epics\n",
    "\n",
    "epics = fetch_epics()\n",
    "\n",
    "# Guardar en archivo si lo deseas\n",
    "with open(\"epics_due_lookup.json\", \"w\") as f:\n",
    "    json.dump(epics, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e65b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '346436483896-non6bmg405eh4avr5saql5m1r0r37rim.apps.googleusercontent.com'\n",
    "client_secret = '346436483896-non6bmg405eh4avr5saql5m1r0r37rim.apps.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Consultando reuniones de Luis Uran (luisuran@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de Diego Martin Gogorza (diegogogorza@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de Nicolas Pardo (nicolaspardo@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de MartÃ­n Horn (martinhorn@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de Facundo Capua (facundocapua@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de Franco Lorenzo (francolorenzo@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de Alan Mori - Carestino (alanmori@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de GastÃ³n Ojeda (gastonojeda@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de Miguel Armentano (miguelarmentano@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de Juan Ignacio Morelis - Carestino (juanmorelis@biamex.com)...\n",
      "ðŸ”Ž Consultando reuniones de ivanalberghini (ivanalberghini@biamex.com)...\n",
      "ðŸ“… Luis Uran: 9 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 09:00\n",
      "ðŸ“… Diego Martin Gogorza: 35 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-01-22 00:00 â†’ 00:00\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-03 11:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-03 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-04 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-05 15:30 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-05 16:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-06 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-06 10:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-06 12:30 â†’ 13:15\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-07 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-07 09:30 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-07 15:15 â†’ 15:45\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-10 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-10 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-10 15:30 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-10 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-11 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 10:30\n",
      "   ðŸ•“ 2025-02-11 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-11 16:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-11 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-12 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 15:30 â†’ 16:30\n",
      "ðŸ“… Nicolas Pardo: 95 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-03 09:00 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-03 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-03 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-03 11:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-03 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-03 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-03 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-03 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-03 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-03 16:00 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-03 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-04 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-04 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-04 09:30 â†’ 17:20\n",
      "   ðŸ•“ 2025-02-04 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-04 11:30 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-04 12:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-04 12:00 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-04 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-04 15:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-04 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-04 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-05 09:00 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-05 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-05 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-05 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-05 12:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-05 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-05 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-05 15:30 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-05 16:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-05 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-06 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-06 09:30 â†’ 17:20\n",
      "   ðŸ•“ 2025-02-06 10:00 â†’ 11:30\n",
      "   ðŸ•“ 2025-02-06 10:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-06 11:30 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-06 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-06 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-06 15:00 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-06 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-07 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-07 09:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-07 09:30 â†’ 17:20\n",
      "   ðŸ•“ 2025-02-07 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-07 11:00 â†’ 11:45\n",
      "   ðŸ•“ 2025-02-07 12:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-07 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-07 15:00 â†’ 15:45\n",
      "   ðŸ•“ 2025-02-07 15:45 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-07 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-10 09:00 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-10 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-10 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-10 12:00 â†’ 14:30\n",
      "   ðŸ•“ 2025-02-10 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-10 16:00 â†’ 16:25\n",
      "   ðŸ•“ 2025-02-10 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-11 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-11 09:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-11 09:30 â†’ 17:20\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 10:30\n",
      "   ðŸ•“ 2025-02-11 11:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-11 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-11 15:30 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-11 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-11 17:30 â†’ 18:30\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-12 09:00 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-12 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-12 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 12:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-12 14:00 â†’ 14:45\n",
      "   ðŸ•“ 2025-02-12 14:45 â†’ 15:15\n",
      "   ðŸ•“ 2025-02-12 15:30 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-12 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-12 16:30 â†’ 17:30\n",
      "ðŸ“… MartÃ­n Horn: 29 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-02-04 12:30 â†’ 13:30\n",
      "   ðŸ•“ 2025-02-04 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 09:15 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-05 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-05 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-05 14:30 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-06 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-06 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-06 15:00 â†’ 15:15\n",
      "   ðŸ•“ 2025-02-06 15:30 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-07 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-07 11:30 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-07 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-07 16:00 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-07 16:30 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-10 11:00 â†’ 12:15\n",
      "   ðŸ•“ 2025-02-10 14:30 â†’ 15:15\n",
      "   ðŸ•“ 2025-02-10 16:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-10 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-11 12:00 â†’ 12:45\n",
      "   ðŸ•“ 2025-02-11 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-12 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 12:00 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-12 14:00 â†’ 14:30\n",
      "   ðŸ•“ 2025-02-12 17:00 â†’ 17:30\n",
      "ðŸ“… Facundo Capua: 0 bloque(s) ocupado(s)\n",
      "ðŸ“… Franco Lorenzo: 34 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-03 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-03 10:00 â†’ 10:50\n",
      "   ðŸ•“ 2025-02-03 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-03 16:30 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-04 08:45 â†’ 09:25\n",
      "   ðŸ•“ 2025-02-04 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-04 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-04 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-05 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-05 10:00 â†’ 10:50\n",
      "   ðŸ•“ 2025-02-05 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-05 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-06 08:45 â†’ 09:25\n",
      "   ðŸ•“ 2025-02-06 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-07 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-10 09:30 â†’ 11:50\n",
      "   ðŸ•“ 2025-02-10 10:00 â†’ 10:50\n",
      "   ðŸ•“ 2025-02-10 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-10 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-10 16:00 â†’ 16:25\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-11 08:45 â†’ 09:25\n",
      "   ðŸ•“ 2025-02-11 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 08:45\n",
      "   ðŸ•“ 2025-02-12 09:30 â†’ 09:55\n",
      "   ðŸ•“ 2025-02-12 10:00 â†’ 10:50\n",
      "   ðŸ•“ 2025-02-12 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-12 15:30 â†’ 16:30\n",
      "ðŸ“… Alan Mori - Carestino: 17 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-03 10:30 â†’ 10:35\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-04 10:30 â†’ 10:35\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 10:30 â†’ 10:35\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-06 10:30 â†’ 10:35\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-07 10:30 â†’ 10:35\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-10 10:30 â†’ 10:35\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-11 10:30 â†’ 10:35\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 10:30 â†’ 10:35\n",
      "ðŸ“… GastÃ³n Ojeda: 41 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-03 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-03 11:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-03 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-04 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-04 09:00 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-04 12:00 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-04 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-05 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-05 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-05 16:00 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-06 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-06 10:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-06 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-06 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-07 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-07 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-07 15:45 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-10 09:30 â†’ 11:50\n",
      "   ðŸ•“ 2025-02-10 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-10 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-10 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-10 16:00 â†’ 16:25\n",
      "   ðŸ•“ 2025-02-11 07:30 â†’ 08:30\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-11 09:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-11 11:30 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-11 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-11 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 09:00 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-12 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-12 15:30 â†’ 16:30\n",
      "ðŸ“… Miguel Armentano: 28 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-02-03 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-03 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-03 16:00 â†’ 16:15\n",
      "   ðŸ•“ 2025-02-04 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-04 09:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-04 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-04 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-05 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-05 13:00 â†’ 14:00\n",
      "   ðŸ•“ 2025-02-05 16:00 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-06 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-06 09:00 â†’ 09:30\n",
      "   ðŸ•“ 2025-02-06 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-07 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-07 09:30 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-07 10:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-07 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-07 15:15 â†’ 15:45\n",
      "   ðŸ•“ 2025-02-07 15:45 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-10 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-10 11:30 â†’ 12:15\n",
      "   ðŸ•“ 2025-02-10 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-10 15:30 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-11 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-11 12:00 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-11 16:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-12 08:30 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 12:00 â†’ 13:00\n",
      "ðŸ“… Juan Ignacio Morelis - Carestino: 43 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-01-27 00:00 â†’ 00:00\n",
      "   ðŸ•“ 2025-02-03 11:30 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-03 12:30 â†’ 13:15\n",
      "   ðŸ•“ 2025-02-03 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-03 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-04 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-04 12:00 â†’ 12:45\n",
      "   ðŸ•“ 2025-02-04 14:00 â†’ 14:30\n",
      "   ðŸ•“ 2025-02-05 09:15 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-05 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-05 16:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-06 10:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-06 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-06 15:00 â†’ 15:45\n",
      "   ðŸ•“ 2025-02-07 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-07 12:00 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-07 15:00 â†’ 15:45\n",
      "   ðŸ•“ 2025-02-10 14:30 â†’ 15:15\n",
      "   ðŸ•“ 2025-02-10 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-10 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-10 15:30 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 10:45\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 10:45\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-11 11:30 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-11 12:00 â†’ 12:45\n",
      "   ðŸ•“ 2025-02-11 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-11 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-11 15:30 â†’ 15:50\n",
      "   ðŸ•“ 2025-02-11 16:00 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-11 16:30 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-12 08:00 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 09:30 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-12 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 12:00 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-12 12:30 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-12 14:00 â†’ 14:30\n",
      "   ðŸ•“ 2025-02-12 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-12 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-12 15:30 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-12 16:30 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-12 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-12 17:30 â†’ 18:30\n",
      "ðŸ“… ivanalberghini: 43 bloque(s) ocupado(s)\n",
      "   ðŸ•“ 2025-01-27 00:00 â†’ 00:00\n",
      "   ðŸ•“ 2025-02-03 11:30 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-03 12:30 â†’ 13:15\n",
      "   ðŸ•“ 2025-02-03 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-03 15:00 â†’ 16:00\n",
      "   ðŸ•“ 2025-02-04 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-04 12:00 â†’ 12:45\n",
      "   ðŸ•“ 2025-02-04 14:00 â†’ 14:30\n",
      "   ðŸ•“ 2025-02-05 09:15 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-05 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-05 16:00 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-06 10:30 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-06 14:00 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-06 15:00 â†’ 15:45\n",
      "   ðŸ•“ 2025-02-07 11:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-07 12:00 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-07 15:00 â†’ 15:45\n",
      "   ðŸ•“ 2025-02-10 14:30 â†’ 15:15\n",
      "   ðŸ•“ 2025-02-10 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-10 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-10 15:30 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 10:45\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 10:45\n",
      "   ðŸ•“ 2025-02-11 10:00 â†’ 11:00\n",
      "   ðŸ•“ 2025-02-11 11:30 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-11 12:00 â†’ 12:45\n",
      "   ðŸ•“ 2025-02-11 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-11 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-11 15:30 â†’ 15:50\n",
      "   ðŸ•“ 2025-02-11 16:00 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-11 16:30 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-12 08:00 â†’ 09:00\n",
      "   ðŸ•“ 2025-02-12 09:30 â†’ 10:00\n",
      "   ðŸ•“ 2025-02-12 10:00 â†’ 12:00\n",
      "   ðŸ•“ 2025-02-12 12:00 â†’ 12:30\n",
      "   ðŸ•“ 2025-02-12 12:30 â†’ 13:00\n",
      "   ðŸ•“ 2025-02-12 14:00 â†’ 14:30\n",
      "   ðŸ•“ 2025-02-12 14:30 â†’ 15:00\n",
      "   ðŸ•“ 2025-02-12 15:00 â†’ 15:30\n",
      "   ðŸ•“ 2025-02-12 15:30 â†’ 16:30\n",
      "   ðŸ•“ 2025-02-12 16:30 â†’ 17:00\n",
      "   ðŸ•“ 2025-02-12 17:00 â†’ 17:30\n",
      "   ðŸ•“ 2025-02-12 17:30 â†’ 18:30\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# ðŸ“Œ Permisos necesarios: lectura del calendario\n",
    "SCOPES = ['https://www.googleapis.com/auth/calendar.readonly']\n",
    "\n",
    "# ðŸ“‚ Archivos de autenticaciÃ³n\n",
    "CREDENTIALS_PATH = 'client_secret.json'\n",
    "TOKEN_PATH = 'token.json'\n",
    "\n",
    "\n",
    "\n",
    "def obtener_credenciales():\n",
    "    creds = None\n",
    "    if os.path.exists(TOKEN_PATH):\n",
    "        creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open(TOKEN_PATH, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    return creds\n",
    "\n",
    "def obtener_bloques_ocupados(email, creds, start_dt, end_dt):\n",
    "    service = build('calendar', 'v3', credentials=creds)\n",
    "    eventos_resultado = service.events().list(\n",
    "        calendarId=email,\n",
    "        timeMin=start_dt.isoformat(),\n",
    "        timeMax=end_dt.isoformat(),\n",
    "        singleEvents=True,\n",
    "        orderBy=\"startTime\"\n",
    "    ).execute()\n",
    "\n",
    "    eventos = eventos_resultado.get('items', [])\n",
    "    bloques_ocupados = []\n",
    "\n",
    "    for evento in eventos:\n",
    "        start_str = evento['start'].get('dateTime')\n",
    "        end_str = evento['end'].get('dateTime')\n",
    "        if not start_str or not end_str:\n",
    "            continue  # Omitir eventos de todo el dÃ­a\n",
    "\n",
    "        start_time = datetime.datetime.fromisoformat(start_str)\n",
    "        end_time = datetime.datetime.fromisoformat(end_str)\n",
    "        bloques_ocupados.append((start_time, end_time))\n",
    "\n",
    "    return bloques_ocupados\n",
    "\n",
    "def obtener_bloques_por_dev():\n",
    "    creds = obtener_credenciales()\n",
    "    bloques_por_dev = {}\n",
    "\n",
    "    for dev, email in MAIL_MAP.items():\n",
    "        print(f\"ðŸ”Ž Consultando reuniones de {dev} ({email})...\")\n",
    "        try:\n",
    "            bloques = obtener_bloques_ocupados(email, creds, DEFAULT_START_DATE_with_timezone, DEFAULT_END_DATE_with_timezone)\n",
    "            bloques_por_dev[dev] = bloques\n",
    "            #print(f\"   âœ… {len(bloques)} reuniones encontradas.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error consultando {email}: {str(e)}\\n\")\n",
    "            bloques_por_dev[dev] = []\n",
    "\n",
    "    return bloques_por_dev\n",
    "\n",
    "# Para usar de forma aislada:\n",
    "if __name__ == \"__main__\":\n",
    "    bloques = obtener_bloques_por_dev()\n",
    "    for dev, bloques_list in bloques.items():\n",
    "        print(f\"ðŸ“… {dev}: {len(bloques_list)} bloque(s) ocupado(s)\")\n",
    "        for start, end in bloques_list:\n",
    "            print(f\"   ðŸ•“ {start.strftime('%Y-%m-%d %H:%M')} â†’ {end.strftime('%H:%M')}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761eeca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Miguel Armentano:\n",
      "  ðŸŽ¯ Epic target: 50.0%\n",
      "  ðŸ“Š Total in range: 42.0 h | Epic in range: 42.0 h\n",
      "  ðŸ” Epic tasks out of range: 7\n",
      "    - IT-20942 (8.0 h) due 2025-10-17\n",
      "    - IT-22564 (8.0 h) due 2025-11-28\n",
      "    - IT-22566 (12.0 h) due 2025-12-19\n",
      "    - IT-22625 (4.0 h) due 2025-12-30\n",
      "    - IT-23213 (6.0 h) due 2026-01-22\n",
      "    - IT-11523 (8.0 h) due 2026-02-20\n",
      "    - IT-11789 (2.0 h) due 2100-01-01\n",
      " modificando IT-20942 due 2100-01-01\n",
      "âœ”ï¸ Ratio tras forzar IT-20942: 50.0 / (50.0) = 100.0%\n",
      "\n",
      "ðŸ”Ž Nicolas Pardo:\n",
      "  ðŸŽ¯ Epic target: 10.0%\n",
      "  ðŸ“Š Total in range: 0.0 h | Epic in range: 0.0 h\n",
      "  ðŸ” Epic tasks out of range: 9\n",
      "    - IT-23050 (70.0 h) due 2025-08-01\n",
      "    - IT-21773 (4.0 h) due 2025-10-14\n",
      "    - IT-23275 (48.0 h) due 2025-12-30\n",
      "    - IT-23272 (40.0 h) due 2025-12-31\n",
      "    - IT-23161 (6.0 h) due 2026-01-15\n",
      "    - IT-23035 (8.0 h) due 2026-01-30\n",
      "    - IT-23207 (24.0 h) due 2026-02-26\n",
      "    - IT-22344 (16.0 h) due 2100-01-01\n",
      "    - IT-22187 (12.0 h) due 2100-01-01\n",
      " modificando IT-23050 due 2100-01-01\n",
      "âœ”ï¸ Ratio tras forzar IT-23050: 70.0 / (80.0) = 87.5%\n",
      "\n",
      "ðŸ”Ž Facundo Capua:\n",
      "  ðŸŽ¯ Epic target: 10.0%\n",
      "  ðŸ“Š Total in range: 0.0 h | Epic in range: 0.0 h\n",
      "  ðŸ” Epic tasks out of range: 0\n",
      "\n",
      "ðŸ”Ž GastÃ³n Ojeda:\n",
      "  ðŸŽ¯ Epic target: 0.0%\n",
      "  ðŸ“Š Total in range: 0.0 h | Epic in range: 0.0 h\n",
      "  ðŸ” Epic tasks out of range: 15\n",
      "    - IT-15010 (12.0 h) due 2024-09-13\n",
      "    - IT-14961 (10.0 h) due 2024-09-13\n",
      "    - IT-22262 (4.0 h) due 2025-12-12\n",
      "    - IT-15941 (6.0 h) due 2025-12-31\n",
      "    - IT-15936 (4.0 h) due 2025-12-31\n",
      "    - IT-15935 (8.0 h) due 2025-12-31\n",
      "    - IT-15922 (16.0 h) due 2025-12-31\n",
      "    - IT-22760 (21.0 h) due 2026-01-02\n",
      "    - IT-22739 (8.0 h) due 2026-01-02\n",
      "    - IT-22741 (8.0 h) due 2026-01-08\n",
      "    - IT-23165 (6.0 h) due 2026-01-27\n",
      "    - IT-15932 (5.0 h) due 2100-01-01\n",
      "    - IT-15931 (1.0 h) due 2100-01-01\n",
      "    - IT-15929 (5.0 h) due 2100-01-01\n",
      "    - IT-15927 (5.0 h) due 2100-01-01\n",
      " modificando IT-15010 due 2100-01-01\n",
      "âœ”ï¸ Ratio tras forzar IT-15010: 12.0 / (50.0) = 24.0%\n",
      "\n",
      "ðŸ”Ž Joaquin Fernandez - Carestino:\n",
      "  ðŸŽ¯ Epic target: 10.0%\n",
      "  ðŸ“Š Total in range: 0.0 h | Epic in range: 0.0 h\n",
      "  ðŸ” Epic tasks out of range: 3\n",
      "    - IT-23241 (4.0 h) due 2026-01-29\n",
      "    - IT-23238 (16.0 h) due 2026-01-29\n",
      "    - IT-23221 (4.0 h) due 2026-01-29\n",
      " modificando IT-23241 due 2026-01-29\n",
      "âœ”ï¸ Ratio tras forzar IT-23241: 4.0 / (60.0) = 6.7%\n",
      " modificando IT-23238 due 2026-01-29\n",
      "âœ”ï¸ Ratio tras forzar IT-23238: 20.0 / (60.0) = 33.3%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Joaquin Fernandez - Carestino'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 435\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# ðŸ” Replanificar solo ese desarrollador\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     scheduled \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scheduled \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeveloper\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m dev]\n\u001b[1;32m--> 435\u001b[0m     scheduled \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m planificar_tareas_para_dev(dev, tasks_by_dev[dev], \u001b[43mbloques_por_dev\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# === RESUMEN FINAL ===\u001b[39;00m\n\u001b[0;32m    439\u001b[0m scheduled_by_dev_and_key \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Joaquin Fernandez - Carestino'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# === LOAD DATA ===\n",
    "with open(\"jira_it_issues.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    issues = json.load(f)[\"issues\"]\n",
    "\n",
    "with open(\"epics_due_lookup.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    epic_due_lookup = json.load(f)\n",
    "\n",
    "scheduled = []\n",
    "tasks_by_dev = {}\n",
    "issue_map = {}\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def _opt_value(v):\n",
    "    \"\"\"Si es opciÃ³n (dict), devuelve .value/.name; si es lista, lista de values; otro -> tal cual.\"\"\"\n",
    "    if isinstance(v, dict):\n",
    "        return v.get(\"value\") or v.get(\"name\")\n",
    "    if isinstance(v, list):\n",
    "        out = []\n",
    "        for x in v:\n",
    "            out.append(x.get(\"value\") or x.get(\"name\") if isinstance(x, dict) else x)\n",
    "        return out\n",
    "    return v\n",
    "\n",
    "def get_cf10209_from_sd_outward(f_fields, only_link_type=None):\n",
    "    \"\"\"\n",
    "    Busca en f_fields['issuelinks'] un outwardIssue cuya key empiece con 'SD-'\n",
    "    y devuelve customfield_10209 (normalizado). Si only_link_type se setea,\n",
    "    filtra por el nombre del tipo de vÃ­nculo (p.ej. 'Problem/Incident').\n",
    "    \"\"\"\n",
    "    links = f_fields.get(\"issuelinks\") or []\n",
    "    for link in links:\n",
    "        if only_link_type and link.get(\"type\", {}).get(\"name\") != only_link_type:\n",
    "            continue\n",
    "        out = link.get(\"outwardIssue\")\n",
    "        if not out:\n",
    "            continue\n",
    "        if not str(out.get(\"key\", \"\")).startswith(\"SD-\"):\n",
    "            continue\n",
    "        lfields = (out.get(\"fields\") or {})\n",
    "        val = _opt_value(lfields.get(\"customfield_10209\"))\n",
    "        if val:\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "def get_epic_key(fields):\n",
    "    \"\"\"\n",
    "    Devuelve la key de la Ã©pica asociada al issue (si existe), probando:\n",
    "    1) customfield_10008 (Epic Link) â€“ clÃ¡sico\n",
    "    2) parent.key si el parent es de tipo 'Epic' â€“ team-managed\n",
    "    3) fields['epic'] (algunas instancias Cloud)\n",
    "    \"\"\"\n",
    "    # 1) Epic Link clÃ¡sico\n",
    "    epic_link = fields.get(\"customfield_10008\")\n",
    "    if isinstance(epic_link, str) and epic_link:\n",
    "        return epic_link\n",
    "\n",
    "    # 2) parent -> Epic\n",
    "    p = fields.get(\"parent\")\n",
    "    if isinstance(p, dict):\n",
    "        p_issuetype = (p.get(\"fields\") or {}).get(\"issuetype\") or p.get(\"issuetype\") or {}\n",
    "        if str(p_issuetype.get(\"name\", \"\")).lower() == \"epic\":\n",
    "            return p.get(\"key\")\n",
    "\n",
    "    # 3) objeto 'epic'\n",
    "    e = fields.get(\"epic\")\n",
    "    if isinstance(e, dict):\n",
    "        return e.get(\"key\") or e.get(\"id\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def contar_dias_laborables(start_date, end_date):\n",
    "    dias = 0\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        if current.weekday() < 5:  # Lunes a Viernes\n",
    "            dias += 1\n",
    "        current += timedelta(days=1)\n",
    "    return dias\n",
    "\n",
    "for issue in issues:\n",
    "    f = issue[\"fields\"]\n",
    "    status = f.get(\"status\", {}).get(\"name\", \"\")\n",
    "    assignee = f.get(\"assignee\")\n",
    "    if not assignee:\n",
    "        continue\n",
    "    dev = assignee[\"displayName\"]\n",
    "    report = f.get(\"reporter\")\n",
    "    if not report:\n",
    "        continue\n",
    "    reporter = report[\"displayName\"]\n",
    "    key = issue[\"key\"]\n",
    "    summary = f.get(\"summary\", \"\")\n",
    "    module_info = f.get(\"customfield_10212\")\n",
    "    module_value = module_info[\"value\"] if module_info else None\n",
    "    estimate_hours = f.get(\"customfield_10608\")\n",
    "\n",
    "    if estimate_hours is None:\n",
    "        estimate_hours = f.get(\"customfield_10016\")\n",
    "\n",
    "    if estimate_hours is None:\n",
    "        estimate_seconds = f.get(\"timetracking\", {}).get(\"originalEstimateSeconds\") or f.get(\"aggregatetimeoriginalestimate\")\n",
    "        estimate_hours = estimate_seconds / 3600 if estimate_seconds else 0\n",
    "    \n",
    "    issue_to_epic = {}\n",
    "    for ekey, edata in epic_due_lookup.items():\n",
    "        for t in edata.get(\"tasks\", []):\n",
    "            k = t.get(\"key\")\n",
    "            if k:\n",
    "                issue_to_epic[k] = ekey\n",
    "    \n",
    "    epic_key = get_epic_key(f) or issue_to_epic.get(key)  # ðŸ‘ˆ fallback desde lookup\n",
    "    epic_obj = epic_due_lookup.get(epic_key) if epic_key else None\n",
    "    epic_due_str = (epic_obj or {}).get(\"due_date\")\n",
    "\n",
    "    # Prefiere due del issue; si no hay, usa el de la Ã©pica\n",
    "    due_str = f.get(\"duedate\") or epic_due_str\n",
    "    try:\n",
    "        due_date = datetime.strptime(due_str, \"%Y-%m-%d\") if due_str else datetime(2100, 1, 1)\n",
    "    except:\n",
    "        due_date = datetime(2100, 1, 1)\n",
    "\n",
    "    epic_name = (epic_obj or {}).get(\"summary\", \"â€” Sin Ã©pica â€”\")\n",
    "\n",
    "    cf10442_raw = f.get(\"customfield_10442\") or []\n",
    "    cf10442_names = [u.get(\"displayName\") for u in cf10442_raw if u.get(\"displayName\")]\n",
    "    suggested_devs = MODULE_DEVS.get(module_value, []) if module_value else []\n",
    "    suggested_users = list(set(suggested_devs + cf10442_names))\n",
    "\n",
    "    if dev in suggested_users:\n",
    "        suggested_users.remove(dev)\n",
    "\n",
    "    can_be_delegated = dev not in suggested_users and bool(suggested_users)\n",
    "    delegation_note = f\"ðŸš¨ Puede derivarse a: {', '.join(suggested_users)}\" if can_be_delegated else \"\"\n",
    "\n",
    "\n",
    "\n",
    "    cf10209_value = get_cf10209_from_sd_outward(\n",
    "        f_fields=f,\n",
    "        only_link_type=\"Problem/Incident\"  # o None si no querÃ©s filtrar por tipo\n",
    "    )\n",
    "    \n",
    "    task = {\n",
    "        \"key\": key,\n",
    "        \"summary\": summary,\n",
    "        \"estimate_hours\": estimate_hours,\n",
    "        \"due_date\": due_date,\n",
    "        \"assignee\": dev,\n",
    "        \"status\": status,\n",
    "        \"due_reason\": None,\n",
    "        \"module\": module_value,\n",
    "        \"suggested_users\": suggested_users,\n",
    "        \"has_epic\": bool(epic_key),\n",
    "        \"epic_name\": epic_name, \n",
    "        \"reporter\": reporter,\n",
    "        \"cf10209\": cf10209_value\n",
    "    }\n",
    "    # ðŸ”§ Nuevo: guardar claves de tareas que bloquean esta\n",
    "    \n",
    "    issue_map[key] = task\n",
    "\n",
    "    \n",
    "    if status in VALID_STATUSES:\n",
    "        tasks_by_dev.setdefault(dev, []).append(task)\n",
    "    graph.add_node(key)\n",
    "    \n",
    "\n",
    "    # for key, task in issue_map.items():\n",
    "    #     blocker_due_dates = [\n",
    "    #         issue_map[b][\"due_date\"]\n",
    "    #         for b in graph.predecessors(key)\n",
    "    #         if b in issue_map and issue_map[b][\"due_date\"]\n",
    "    #     ]\n",
    "\n",
    "    #     if blocker_due_dates:\n",
    "    #         max_blocker_due = min(blocker_due_dates)\n",
    "    #         if task[\"due_date\"] < max_blocker_due:\n",
    "    #             task[\"due_date_original\"] = task[\"due_date\"]\n",
    "    #             task[\"due_date\"] = max_blocker_due\n",
    "\n",
    "    for link in f.get(\"issuelinks\", []):\n",
    "        if \"inwardIssue\" in link and link[\"type\"][\"name\"] == \"Blocks\":\n",
    "            graph.add_edge(link[\"inwardIssue\"][\"key\"], key)\n",
    "        elif \"outwardIssue\" in link and link[\"type\"][\"name\"] == \"Blocks\":\n",
    "            graph.add_edge(key, link[\"outwardIssue\"][\"key\"])\n",
    "\n",
    "    blockers = [src for src, tgt in graph.edges() if tgt == key]\n",
    "    task[\"blockers\"] = blockers\n",
    "\n",
    "for key, task in issue_map.items():\n",
    "    # Para cada tarea, mirar a quiÃ©n bloquea\n",
    "    blocked_keys = list(graph.successors(key))\n",
    "    \n",
    "    blocked_due_dates = [\n",
    "        issue_map[b][\"due_date\"]\n",
    "        for b in blocked_keys\n",
    "        if b in issue_map and issue_map[b][\"due_date\"]\n",
    "    ]\n",
    "\n",
    "    blocker_key = [issue_map[b][\"key\"] for b in blocked_keys if b in issue_map and issue_map[b][\"key\"]]\n",
    "\n",
    "    if blocked_due_dates:\n",
    "        min_blocked_due = min(blocked_due_dates)\n",
    "        suggested_due = min_blocked_due - timedelta(days=1)\n",
    "        task[\"note\"] = (\n",
    "            f\"ðŸ•“ Fecha de vencimiento {'ajustada' if task['due_date'] > suggested_due else 'ya correcta'} \"\n",
    "            f\"por bloqueo a tarjeta {blocker_key} que vence el: {suggested_due.strftime('%Y-%m-%d')}\"\n",
    "        )\n",
    "        if task[\"due_date\"] > suggested_due:\n",
    "            task[\"due_date_original\"] = task[\"due_date\"]\n",
    "            task[\"due_date\"] = suggested_due\n",
    "            task[\"note\"] = f\"ðŸ•“ Fecha de vencimiento ajustada por bloqueo a tarjeta {blocker_key} que vence el: {suggested_due.strftime('%Y-%m-%d')}\"\n",
    "\n",
    "\n",
    "# BLOQUES POR DEV (simulados)\n",
    "bloques_por_dev = bloques\n",
    "MINIMUM_SLOT_HOURS = 0.25\n",
    "# === PLANIFICADOR POR DESARROLLADOR ===\n",
    "def to_naive(dt):\n",
    "    return dt.replace(tzinfo=None) if dt.tzinfo else dt\n",
    "\n",
    "def planificar_tareas_para_dev(dev, tasks, bloques_ocupados):\n",
    "    current_time = datetime.combine(DEFAULT_START_DATE.date(), datetime.strptime(\"08:30\", \"%H:%M\").time())\n",
    "    hours_per_day = DAILY_HOURS.get(dev, DAILY_HOURS[\"Default\"])\n",
    "    schedule = {}\n",
    "    plan = []\n",
    "\n",
    "    try:\n",
    "        sorted_keys = list(nx.topological_sort(graph.subgraph([t[\"key\"] for t in tasks])))\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        sorted_keys = [t[\"key\"] for t in tasks]\n",
    "    sorted_keys.sort(key=lambda k: issue_map[k][\"due_date\"])\n",
    "\n",
    "    def to_naive(dt):\n",
    "        return dt.replace(tzinfo=None) if dt.tzinfo else dt\n",
    "    \n",
    "    for start, end in sorted(bloques_ocupados):\n",
    "            plan.append({\n",
    "                \"developer\": dev,\n",
    "                \"key\": f\"REUNION-{start.strftime('%Y%m%d%H%M')}\",\n",
    "                \"summary\": \"â›” ReuniÃ³n\",\n",
    "                \"has_epic\": False,\n",
    "                \"due_date\": start.date(),  # o end.date()\n",
    "                \"start\": to_naive(start),\n",
    "                \"end\": to_naive(end),\n",
    "                \"duration_hours\": (to_naive(end) - to_naive(start)).total_seconds() / 3600,\n",
    "                \"type\": \"reunion\"\n",
    "            })\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        task = issue_map[key]\n",
    "        hours_left = task[\"estimate_hours\"]\n",
    "        start_time = None\n",
    "       \n",
    "\n",
    "        \n",
    "\n",
    "        while hours_left > 0:\n",
    "            date_str = current_time.strftime('%Y-%m-%d')\n",
    "            used_today = schedule.get(date_str, 0)\n",
    "\n",
    "            end_of_day = datetime.combine(current_time.date(), datetime.strptime(\"17:30\", \"%H:%M\").time())\n",
    "            available_time = (end_of_day - current_time).total_seconds() / 3600\n",
    "            time_slot = min(available_time, hours_per_day - used_today, hours_left)\n",
    "\n",
    "             # ðŸ”§ NUEVO BLOQUE: limitar time_slot al hueco libre real antes del prÃ³ximo conflicto\n",
    "            next_conflict_start = None\n",
    "            for start, end in sorted(bloques_ocupados):\n",
    "                if to_naive(start) > to_naive(current_time):\n",
    "                    next_conflict_start = to_naive(start)\n",
    "                    break\n",
    "\n",
    "            if next_conflict_start:\n",
    "                gap_hours = (next_conflict_start - current_time).total_seconds() / 3600\n",
    "                if gap_hours > 0:\n",
    "                    time_slot = min(time_slot, gap_hours)\n",
    "\n",
    "            if time_slot <= 0:\n",
    "                next_day = current_time.date() + timedelta(days=1)\n",
    "                while next_day.weekday() in [5, 6]:\n",
    "                    next_day += timedelta(days=1)\n",
    "                current_time = datetime.combine(next_day, datetime.strptime(\"08:30\", \"%H:%M\").time())\n",
    "                continue\n",
    "\n",
    "            proposed_end = current_time + timedelta(hours=time_slot)\n",
    "\n",
    "            bloque_conflictivo = any(\n",
    "                to_naive(start) < to_naive(proposed_end) and to_naive(end) > to_naive(current_time)\n",
    "                for start, end in bloques_ocupados\n",
    "            )\n",
    "\n",
    "            if bloque_conflictivo:\n",
    "                conflictos = [\n",
    "                    (start, end) for start, end in bloques_ocupados\n",
    "                    if to_naive(start) < to_naive(proposed_end) and to_naive(end) > to_naive(current_time)\n",
    "                ]\n",
    "                conflictos.sort(key=lambda x: to_naive(x[0]))\n",
    "                next_start = to_naive(conflictos[0][1])\n",
    "                current_time = next_start\n",
    "                continue\n",
    "\n",
    "            if start_time is None:\n",
    "                start_time = current_time\n",
    "\n",
    "            if bloque_conflictivo:\n",
    "                conflictos = [\n",
    "                    (start, end) for start, end in bloques_ocupados\n",
    "                    if to_naive(start) < to_naive(proposed_end) and to_naive(end) > to_naive(current_time)\n",
    "                ]\n",
    "                conflictos.sort(key=lambda x: to_naive(x[0]))\n",
    "                next_start = to_naive(conflictos[0][1])\n",
    "                current_time = next_start\n",
    "                continue\n",
    "\n",
    "            if start_time is None:\n",
    "                start_time = current_time\n",
    "\n",
    "            end_time = current_time + timedelta(hours=time_slot)\n",
    "            plan.append({\n",
    "                \"developer\": dev,\n",
    "                \"key\": key,\n",
    "                \"summary\": task[\"summary\"],\n",
    "                \"has_epic\": task[\"has_epic\"],\n",
    "                \"epic_name\": task[\"epic_name\"],\n",
    "                \"due_date\": task[\"due_date\"],\n",
    "                \"start\": current_time,\n",
    "                \"end\": end_time,\n",
    "                \"duration_hours\": time_slot,\n",
    "                \"suggested_users\": task.get(\"suggested_users\", []),\n",
    "                \"blockers\": task.get(\"blockers\", []),\n",
    "                \"note\": task.get(\"note\", \"\"),\n",
    "                \"reporter\": task.get(\"reporter\", \"\"),\n",
    "                \"cf10209\": task.get(\"cf10209\")\n",
    "            })\n",
    "            \n",
    "\n",
    "            schedule[date_str] = used_today + time_slot\n",
    "            hours_left -= time_slot\n",
    "            current_time = end_time\n",
    "\n",
    "    return plan\n",
    "\n",
    "# === PLANIFICACIÃ“N INICIAL ===\n",
    "# for dev in tasks_by_dev:\n",
    "#     scheduled += planificar_tareas_para_dev(dev, tasks_by_dev[dev], bloques_por_dev[dev])\n",
    "\n",
    "# === REBALANCEO ===\n",
    "planned_hours_by_dev = defaultdict(float)\n",
    "project_hours_by_dev = defaultdict(float)\n",
    "\n",
    "for s in scheduled:\n",
    "    if s.get(\"type\") == \"reunion\":\n",
    "        continue\n",
    "    planned_hours_by_dev[s[\"developer\"]] += s[\"duration_hours\"]\n",
    "    if issue_map[s[\"key\"]][\"has_epic\"]:\n",
    "        project_hours_by_dev[s[\"developer\"]] += s[\"duration_hours\"]\n",
    "\n",
    "for dev in tasks_by_dev:\n",
    "    all_tasks = tasks_by_dev[dev]\n",
    "    \n",
    "    within_range = [t for t in all_tasks if DEFAULT_START_DATE <= t[\"due_date\"] <= DEFAULT_END_DATE]\n",
    "    epic_tasks_in_range = sorted([t for t in within_range if t[\"has_epic\"]], key=lambda t: t[\"due_date\"])\n",
    "    non_epic_tasks_in_range = sorted([t for t in within_range if not t[\"has_epic\"]], key=lambda t: t[\"due_date\"], reverse=True)\n",
    "    epic_tasks_out_of_range = sorted(\n",
    "        [t for t in all_tasks if t[\"has_epic\"] and t not in within_range],\n",
    "        key=lambda t: t[\"due_date\"]\n",
    "    )\n",
    "\n",
    "    total_in_range = sum(t[\"estimate_hours\"] for t in within_range)\n",
    "    current_epic = sum(t[\"estimate_hours\"] for t in epic_tasks_in_range)\n",
    "    non_epic_hours = sum(t[\"estimate_hours\"] for t in non_epic_tasks_in_range)\n",
    "    min_ratio = MIN_PROJECT_RATIO.get(dev, MIN_PROJECT_RATIO[\"Default\"])\n",
    "\n",
    "    print(f\"\\nðŸ”Ž {dev}:\")\n",
    "    print(f\"  ðŸŽ¯ Epic target: {min_ratio * 100:.1f}%\")\n",
    "    print(f\"  ðŸ“Š Total in range: {total_in_range:.1f} h | Epic in range: {current_epic:.1f} h\")\n",
    "    print(f\"  ðŸ” Epic tasks out of range: {len(epic_tasks_out_of_range)}\")\n",
    "    for t in epic_tasks_out_of_range:\n",
    "        print(f\"    - {t['key']} ({t['estimate_hours']} h) due {t['due_date'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # ðŸŸ¨ Forzar tareas con Ã©pica fuera de rango hasta cumplir el mÃ­nimo\n",
    "    # horas_planificables_en_rango = (\n",
    "    #         sum(t[\"estimate_hours\"] for t in epic_tasks_in_range) +\n",
    "    #         sum(t[\"estimate_hours\"] for t in non_epic_tasks_in_range) +\n",
    "    #         sum(t[\"estimate_hours\"] for t in epic_tasks_out_of_range)\n",
    "    #     )\n",
    "    \n",
    "    dias_laborables = contar_dias_laborables(DEFAULT_START_DATE, DEFAULT_END_DATE)\n",
    "    horas_por_dia = DAILY_HOURS.get(dev, DAILY_HOURS[\"Default\"])\n",
    "    horas_planificables_en_rango = dias_laborables * horas_por_dia\n",
    "    while epic_tasks_out_of_range:\n",
    "        task = epic_tasks_out_of_range.pop(0)\n",
    "        task_estimate = task[\"estimate_hours\"]\n",
    "\n",
    "        # Moverla dentro del rango\n",
    "        task[\"due_date_original\"] = task[\"due_date\"]\n",
    "        print(f\" modificando {task['key']} due {t['due_date'].strftime('%Y-%m-%d')}\")\n",
    "        task[\"due_date\"] = DEFAULT_START_DATE\n",
    "        epic_tasks_in_range.append(task)\n",
    "        current_epic += task_estimate\n",
    "\n",
    "        # Recalcular ratio despuÃ©s de agregarla\n",
    "        ratio_actual = current_epic / horas_planificables_en_rango if horas_planificables_en_rango > 0 else 0\n",
    "        print(f\"âœ”ï¸ Ratio tras forzar {task['key']}: {current_epic:.1f} / ({horas_planificables_en_rango:.1f}) = {ratio_actual * 100:.1f}%\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if ratio_actual >= min_ratio:\n",
    "            break\n",
    "\n",
    "    # âŒ Eliminar tareas sin Ã©pica si sigue sin cumplirse el mÃ­nimo\n",
    "    ratio_actual = current_epic / (current_epic + non_epic_hours) if (current_epic + non_epic_hours) > 0 else 0\n",
    "    while non_epic_tasks_in_range and ratio_actual < min_ratio:\n",
    "        removed = non_epic_tasks_in_range.pop()\n",
    "        non_epic_hours -= removed[\"estimate_hours\"]\n",
    "        ratio_actual = current_epic / (current_epic + non_epic_hours) if (current_epic + non_epic_hours) > 0 else 0\n",
    "\n",
    "    # ðŸ”„ Reordenar: primero Ã©picas, luego tareas normales\n",
    "    balanced_tasks = sorted(epic_tasks_in_range, key=lambda t: t[\"due_date\"]) + \\\n",
    "                     sorted(non_epic_tasks_in_range, key=lambda t: t[\"due_date\"])\n",
    "    leftovers = [t for t in all_tasks if t not in balanced_tasks]\n",
    "    tasks_by_dev[dev] = balanced_tasks + leftovers\n",
    "\n",
    "    # ðŸ” Replanificar solo ese desarrollador\n",
    "    scheduled = [s for s in scheduled if s[\"developer\"] != dev]\n",
    "    scheduled += planificar_tareas_para_dev(dev, tasks_by_dev[dev], bloques_por_dev[dev])\n",
    "\n",
    "\n",
    "# === RESUMEN FINAL ===\n",
    "scheduled_by_dev_and_key = defaultdict(lambda: defaultdict(list))\n",
    "for s in scheduled:\n",
    "    scheduled_by_dev_and_key[s[\"developer\"]][s[\"key\"]].append(s)\n",
    "\n",
    "print(\"\\nðŸ“ Developer Task Schedule Summary:\\n\")\n",
    "for dev, tasks in tasks_by_dev.items():\n",
    "    print(f\"ðŸ‘¨â€ðŸ’» Developer: {dev}\")\n",
    "    try:\n",
    "        sorted_keys = list(nx.topological_sort(graph.subgraph([t[\"key\"] for t in tasks])))\n",
    "    except nx.NetworkXUnfeasible:\n",
    "        sorted_keys = [t[\"key\"] for t in tasks]\n",
    "    sorted_keys.sort(key=lambda k: issue_map[k][\"due_date\"])\n",
    "    for key in sorted_keys:\n",
    "        task = issue_map[key]\n",
    "        scheds = sorted(scheduled_by_dev_and_key[dev].get(key, []), key=lambda x: x[\"start\"])\n",
    "        if not scheds:\n",
    "            continue\n",
    "        start_time = scheds[0][\"start\"]\n",
    "        end_time = scheds[-1][\"end\"]\n",
    "        total_hours = sum(s[\"duration_hours\"] for s in scheds)\n",
    "        note_lines = []\n",
    "        if \"note\" in task and task[\"note\"]:\n",
    "            note_lines.append(task[\"note\"])\n",
    "\n",
    "        # ðŸš¨ Sugerencias de derivaciÃ³n\n",
    "        if task.get(\"suggested_users\"):\n",
    "            note_lines.append(f\"ðŸš¨ Puede derivarse a: {', '.join(task['suggested_users'])}\")\n",
    "\n",
    "        # ðŸ“› Bloqueadores\n",
    "        if task.get(\"blockers\"):\n",
    "            blocker_msgs = []\n",
    "            for blocker_key in task[\"blockers\"]:\n",
    "                blocker = issue_map.get(blocker_key)\n",
    "                if blocker:\n",
    "                    blocker_msgs.append(f\"{blocker_key}: {blocker['summary']} ({blocker['assignee']}, Due: {blocker['due_date'].strftime('%Y-%m-%d')})\")\n",
    "                else:\n",
    "                    blocker_msgs.append(blocker_key)\n",
    "            note_lines.append(f\"ðŸ“› Debido a bloqueo de: \" + \" | \".join(blocker_msgs))\n",
    "\n",
    "         # Agregar nota personalizada si existe\n",
    "        if \"note\" in task and task[\"note\"]:\n",
    "            note_lines.append(task[\"note\"])\n",
    "\n",
    "        note = \"\\n     \" + \"\\n     \".join(note_lines) if note_lines else \"\"\n",
    "        print(\n",
    "            f\"   ðŸ”¹ {key}: {round(total_hours, 1)}h â†’ Due: {task['due_date'].strftime('%Y-%m-%d')} | \"\n",
    "            f\"Finish: Start: {start_time.strftime('%Y-%m-%d %H:%M')} | End: {end_time.strftime('%Y-%m-%d %H:%M')}{note}\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"planificacion.json\", \"w\") as f:\n",
    "    json.dump(scheduled, f, default=str, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
